{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5194897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본 프로젝트를 진행하는 데 필요한 라이브러리를 가져온다.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64eee327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) 데이터 가져오기\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "df_X = diabetes.data\n",
    "df_y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a25dd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "         0.01990842, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "        -0.06832974, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "         0.00286377, -0.02593034],\n",
       "       ...,\n",
       "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "        -0.04687948,  0.01549073],\n",
       "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "         0.04452837, -0.02593034],\n",
       "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "        -0.00421986,  0.00306441]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2) 모델에 입력할 데이터 X 준비하기\n",
    "df_X = np.array(df_X)\n",
    "df_X\n",
    "# df_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867cc93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "       220.,  57.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (3) 모델에 예측할 데이터 y 준비하기\n",
    "df_y = np.array(df_y)\n",
    "df_y\n",
    "#df_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03bdf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 10) (353,)\n",
      "(89, 10) (89,)\n"
     ]
    }
   ],
   "source": [
    "# (4) train 데이터와 test 데이터로 분리하기\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c00b55b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) 모델 준비하기\n",
    "W = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "# print(W, b)\n",
    "\n",
    "def model(X, W, b):\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2438d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) 손실함수 loss 정의하기\n",
    "def MSE(a, b):\n",
    "    mse = ((a - b) ** 2).mean()  # 두 값의 차이의 제곱의 평균\n",
    "    return mse\n",
    "\n",
    "def loss(X, W, b, y):\n",
    "    predictions = model(X, W, b)\n",
    "    L = MSE(predictions, y)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd9dd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7) 기울기를 구하는 gradient 함수 구현하기\n",
    "def gradient(X, W, b, y):\n",
    "    # N은 데이터 포인트의 개수\n",
    "    N = len(y)\n",
    "\n",
    "    # y_pred 준비\n",
    "    y_pred = model(X, W, b)\n",
    "\n",
    "    # 공식에 맞게 gradient 계산\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "\n",
    "    # b의 gradient 계산\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db\n",
    "\n",
    "# dW, db = gradient(X, W, b, y)\n",
    "# print(\"dW:\", dW)\n",
    "# print(\"db:\", db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bad2ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (8) 하이퍼 파라미터인 학습률 설정하기\n",
    "LEARNING_RATE = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "964c07f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 12897.2863\n",
      "Iteration 20 : Loss 8007.5251\n",
      "Iteration 30 : Loss 6572.4785\n",
      "Iteration 40 : Loss 6139.8049\n",
      "Iteration 50 : Loss 5998.0754\n",
      "Iteration 60 : Loss 5940.9021\n",
      "Iteration 70 : Loss 5908.4101\n",
      "Iteration 80 : Loss 5883.2280\n",
      "Iteration 90 : Loss 5860.3148\n",
      "Iteration 100 : Loss 5838.2067\n",
      "Iteration 110 : Loss 5816.4773\n",
      "Iteration 120 : Loss 5795.0016\n",
      "Iteration 130 : Loss 5773.7418\n",
      "Iteration 140 : Loss 5752.6854\n",
      "Iteration 150 : Loss 5731.8274\n",
      "Iteration 160 : Loss 5711.1649\n",
      "Iteration 170 : Loss 5690.6957\n",
      "Iteration 180 : Loss 5670.4176\n",
      "Iteration 190 : Loss 5650.3287\n",
      "Iteration 200 : Loss 5630.4270\n",
      "Iteration 210 : Loss 5610.7107\n",
      "Iteration 220 : Loss 5591.1777\n",
      "Iteration 230 : Loss 5571.8263\n",
      "Iteration 240 : Loss 5552.6545\n",
      "Iteration 250 : Loss 5533.6605\n",
      "Iteration 260 : Loss 5514.8425\n",
      "Iteration 270 : Loss 5496.1988\n",
      "Iteration 280 : Loss 5477.7274\n",
      "Iteration 290 : Loss 5459.4266\n",
      "Iteration 300 : Loss 5441.2947\n",
      "Iteration 310 : Loss 5423.3300\n",
      "Iteration 320 : Loss 5405.5307\n",
      "Iteration 330 : Loss 5387.8952\n",
      "Iteration 340 : Loss 5370.4217\n",
      "Iteration 350 : Loss 5353.1087\n",
      "Iteration 360 : Loss 5335.9544\n",
      "Iteration 370 : Loss 5318.9573\n",
      "Iteration 380 : Loss 5302.1158\n",
      "Iteration 390 : Loss 5285.4282\n",
      "Iteration 400 : Loss 5268.8930\n",
      "Iteration 410 : Loss 5252.5086\n",
      "Iteration 420 : Loss 5236.2735\n",
      "Iteration 430 : Loss 5220.1862\n",
      "Iteration 440 : Loss 5204.2452\n",
      "Iteration 450 : Loss 5188.4489\n",
      "Iteration 460 : Loss 5172.7960\n",
      "Iteration 470 : Loss 5157.2848\n",
      "Iteration 480 : Loss 5141.9141\n",
      "Iteration 490 : Loss 5126.6824\n",
      "Iteration 500 : Loss 5111.5882\n",
      "Iteration 510 : Loss 5096.6302\n",
      "Iteration 520 : Loss 5081.8070\n",
      "Iteration 530 : Loss 5067.1171\n",
      "Iteration 540 : Loss 5052.5593\n",
      "Iteration 550 : Loss 5038.1323\n",
      "Iteration 560 : Loss 5023.8346\n",
      "Iteration 570 : Loss 5009.6650\n",
      "Iteration 580 : Loss 4995.6222\n",
      "Iteration 590 : Loss 4981.7049\n",
      "Iteration 600 : Loss 4967.9118\n",
      "Iteration 610 : Loss 4954.2417\n",
      "Iteration 620 : Loss 4940.6932\n",
      "Iteration 630 : Loss 4927.2653\n",
      "Iteration 640 : Loss 4913.9567\n",
      "Iteration 650 : Loss 4900.7661\n",
      "Iteration 660 : Loss 4887.6924\n",
      "Iteration 670 : Loss 4874.7344\n",
      "Iteration 680 : Loss 4861.8910\n",
      "Iteration 690 : Loss 4849.1609\n",
      "Iteration 700 : Loss 4836.5430\n",
      "Iteration 710 : Loss 4824.0363\n",
      "Iteration 720 : Loss 4811.6395\n",
      "Iteration 730 : Loss 4799.3516\n",
      "Iteration 740 : Loss 4787.1715\n",
      "Iteration 750 : Loss 4775.0981\n",
      "Iteration 760 : Loss 4763.1303\n",
      "Iteration 770 : Loss 4751.2671\n",
      "Iteration 780 : Loss 4739.5073\n",
      "Iteration 790 : Loss 4727.8501\n",
      "Iteration 800 : Loss 4716.2943\n",
      "Iteration 810 : Loss 4704.8389\n",
      "Iteration 820 : Loss 4693.4830\n",
      "Iteration 830 : Loss 4682.2255\n",
      "Iteration 840 : Loss 4671.0654\n",
      "Iteration 850 : Loss 4660.0018\n",
      "Iteration 860 : Loss 4649.0336\n",
      "Iteration 870 : Loss 4638.1601\n",
      "Iteration 880 : Loss 4627.3801\n",
      "Iteration 890 : Loss 4616.6928\n",
      "Iteration 900 : Loss 4606.0972\n",
      "Iteration 910 : Loss 4595.5925\n",
      "Iteration 920 : Loss 4585.1777\n",
      "Iteration 930 : Loss 4574.8519\n",
      "Iteration 940 : Loss 4564.6143\n",
      "Iteration 950 : Loss 4554.4639\n",
      "Iteration 960 : Loss 4544.3999\n",
      "Iteration 970 : Loss 4534.4215\n",
      "Iteration 980 : Loss 4524.5277\n",
      "Iteration 990 : Loss 4514.7178\n",
      "Iteration 1000 : Loss 4504.9908\n",
      "Iteration 1010 : Loss 4495.3461\n",
      "Iteration 1020 : Loss 4485.7828\n",
      "Iteration 1030 : Loss 4476.3000\n",
      "Iteration 1040 : Loss 4466.8969\n",
      "Iteration 1050 : Loss 4457.5729\n",
      "Iteration 1060 : Loss 4448.3271\n",
      "Iteration 1070 : Loss 4439.1586\n",
      "Iteration 1080 : Loss 4430.0669\n",
      "Iteration 1090 : Loss 4421.0510\n",
      "Iteration 1100 : Loss 4412.1103\n",
      "Iteration 1110 : Loss 4403.2440\n",
      "Iteration 1120 : Loss 4394.4514\n",
      "Iteration 1130 : Loss 4385.7318\n",
      "Iteration 1140 : Loss 4377.0844\n",
      "Iteration 1150 : Loss 4368.5085\n",
      "Iteration 1160 : Loss 4360.0034\n",
      "Iteration 1170 : Loss 4351.5684\n",
      "Iteration 1180 : Loss 4343.2029\n",
      "Iteration 1190 : Loss 4334.9062\n",
      "Iteration 1200 : Loss 4326.6775\n",
      "Iteration 1210 : Loss 4318.5162\n",
      "Iteration 1220 : Loss 4310.4217\n",
      "Iteration 1230 : Loss 4302.3933\n",
      "Iteration 1240 : Loss 4294.4304\n",
      "Iteration 1250 : Loss 4286.5322\n",
      "Iteration 1260 : Loss 4278.6983\n",
      "Iteration 1270 : Loss 4270.9279\n",
      "Iteration 1280 : Loss 4263.2204\n",
      "Iteration 1290 : Loss 4255.5752\n",
      "Iteration 1300 : Loss 4247.9918\n",
      "Iteration 1310 : Loss 4240.4695\n",
      "Iteration 1320 : Loss 4233.0077\n",
      "Iteration 1330 : Loss 4225.6058\n",
      "Iteration 1340 : Loss 4218.2633\n",
      "Iteration 1350 : Loss 4210.9795\n",
      "Iteration 1360 : Loss 4203.7540\n",
      "Iteration 1370 : Loss 4196.5861\n",
      "Iteration 1380 : Loss 4189.4753\n",
      "Iteration 1390 : Loss 4182.4209\n",
      "Iteration 1400 : Loss 4175.4226\n",
      "Iteration 1410 : Loss 4168.4797\n",
      "Iteration 1420 : Loss 4161.5917\n",
      "Iteration 1430 : Loss 4154.7581\n",
      "Iteration 1440 : Loss 4147.9784\n",
      "Iteration 1450 : Loss 4141.2519\n",
      "Iteration 1460 : Loss 4134.5783\n",
      "Iteration 1470 : Loss 4127.9570\n",
      "Iteration 1480 : Loss 4121.3875\n",
      "Iteration 1490 : Loss 4114.8693\n",
      "Iteration 1500 : Loss 4108.4019\n",
      "Iteration 1510 : Loss 4101.9849\n",
      "Iteration 1520 : Loss 4095.6177\n",
      "Iteration 1530 : Loss 4089.2998\n",
      "Iteration 1540 : Loss 4083.0309\n",
      "Iteration 1550 : Loss 4076.8104\n",
      "Iteration 1560 : Loss 4070.6378\n",
      "Iteration 1570 : Loss 4064.5128\n",
      "Iteration 1580 : Loss 4058.4348\n",
      "Iteration 1590 : Loss 4052.4034\n",
      "Iteration 1600 : Loss 4046.4182\n",
      "Iteration 1610 : Loss 4040.4787\n",
      "Iteration 1620 : Loss 4034.5845\n",
      "Iteration 1630 : Loss 4028.7351\n",
      "Iteration 1640 : Loss 4022.9302\n",
      "Iteration 1650 : Loss 4017.1692\n",
      "Iteration 1660 : Loss 4011.4519\n",
      "Iteration 1670 : Loss 4005.7778\n",
      "Iteration 1680 : Loss 4000.1464\n",
      "Iteration 1690 : Loss 3994.5574\n",
      "Iteration 1700 : Loss 3989.0104\n",
      "Iteration 1710 : Loss 3983.5049\n",
      "Iteration 1720 : Loss 3978.0406\n",
      "Iteration 1730 : Loss 3972.6171\n",
      "Iteration 1740 : Loss 3967.2340\n",
      "Iteration 1750 : Loss 3961.8910\n",
      "Iteration 1760 : Loss 3956.5876\n",
      "Iteration 1770 : Loss 3951.3234\n",
      "Iteration 1780 : Loss 3946.0982\n",
      "Iteration 1790 : Loss 3940.9115\n",
      "Iteration 1800 : Loss 3935.7631\n",
      "Iteration 1810 : Loss 3930.6524\n",
      "Iteration 1820 : Loss 3925.5792\n",
      "Iteration 1830 : Loss 3920.5432\n",
      "Iteration 1840 : Loss 3915.5439\n",
      "Iteration 1850 : Loss 3910.5811\n",
      "Iteration 1860 : Loss 3905.6543\n",
      "Iteration 1870 : Loss 3900.7634\n",
      "Iteration 1880 : Loss 3895.9078\n",
      "Iteration 1890 : Loss 3891.0874\n",
      "Iteration 1900 : Loss 3886.3017\n",
      "Iteration 1910 : Loss 3881.5505\n",
      "Iteration 1920 : Loss 3876.8334\n",
      "Iteration 1930 : Loss 3872.1502\n",
      "Iteration 1940 : Loss 3867.5004\n",
      "Iteration 1950 : Loss 3862.8839\n",
      "Iteration 1960 : Loss 3858.3003\n",
      "Iteration 1970 : Loss 3853.7492\n",
      "Iteration 1980 : Loss 3849.2304\n",
      "Iteration 1990 : Loss 3844.7436\n",
      "Iteration 2000 : Loss 3840.2886\n",
      "Iteration 2010 : Loss 3835.8649\n",
      "Iteration 2020 : Loss 3831.4724\n",
      "Iteration 2030 : Loss 3827.1107\n",
      "Iteration 2040 : Loss 3822.7795\n",
      "Iteration 2050 : Loss 3818.4786\n",
      "Iteration 2060 : Loss 3814.2078\n",
      "Iteration 2070 : Loss 3809.9666\n",
      "Iteration 2080 : Loss 3805.7549\n",
      "Iteration 2090 : Loss 3801.5724\n",
      "Iteration 2100 : Loss 3797.4189\n",
      "Iteration 2110 : Loss 3793.2940\n",
      "Iteration 2120 : Loss 3789.1974\n",
      "Iteration 2130 : Loss 3785.1291\n",
      "Iteration 2140 : Loss 3781.0886\n",
      "Iteration 2150 : Loss 3777.0758\n",
      "Iteration 2160 : Loss 3773.0903\n",
      "Iteration 2170 : Loss 3769.1320\n",
      "Iteration 2180 : Loss 3765.2007\n",
      "Iteration 2190 : Loss 3761.2959\n",
      "Iteration 2200 : Loss 3757.4176\n",
      "Iteration 2210 : Loss 3753.5654\n",
      "Iteration 2220 : Loss 3749.7393\n",
      "Iteration 2230 : Loss 3745.9388\n",
      "Iteration 2240 : Loss 3742.1638\n",
      "Iteration 2250 : Loss 3738.4140\n",
      "Iteration 2260 : Loss 3734.6893\n",
      "Iteration 2270 : Loss 3730.9895\n",
      "Iteration 2280 : Loss 3727.3141\n",
      "Iteration 2290 : Loss 3723.6632\n",
      "Iteration 2300 : Loss 3720.0364\n",
      "Iteration 2310 : Loss 3716.4336\n",
      "Iteration 2320 : Loss 3712.8545\n",
      "Iteration 2330 : Loss 3709.2989\n",
      "Iteration 2340 : Loss 3705.7666\n",
      "Iteration 2350 : Loss 3702.2574\n",
      "Iteration 2360 : Loss 3698.7712\n",
      "Iteration 2370 : Loss 3695.3076\n",
      "Iteration 2380 : Loss 3691.8665\n",
      "Iteration 2390 : Loss 3688.4478\n",
      "Iteration 2400 : Loss 3685.0511\n",
      "Iteration 2410 : Loss 3681.6764\n",
      "Iteration 2420 : Loss 3678.3234\n",
      "Iteration 2430 : Loss 3674.9919\n",
      "Iteration 2440 : Loss 3671.6818\n",
      "Iteration 2450 : Loss 3668.3928\n",
      "Iteration 2460 : Loss 3665.1248\n",
      "Iteration 2470 : Loss 3661.8776\n",
      "Iteration 2480 : Loss 3658.6510\n",
      "Iteration 2490 : Loss 3655.4449\n",
      "Iteration 2500 : Loss 3652.2590\n",
      "Iteration 2510 : Loss 3649.0932\n",
      "Iteration 2520 : Loss 3645.9473\n",
      "Iteration 2530 : Loss 3642.8212\n",
      "Iteration 2540 : Loss 3639.7146\n",
      "Iteration 2550 : Loss 3636.6274\n",
      "Iteration 2560 : Loss 3633.5595\n",
      "Iteration 2570 : Loss 3630.5106\n",
      "Iteration 2580 : Loss 3627.4806\n",
      "Iteration 2590 : Loss 3624.4694\n",
      "Iteration 2600 : Loss 3621.4768\n",
      "Iteration 2610 : Loss 3618.5026\n",
      "Iteration 2620 : Loss 3615.5467\n",
      "Iteration 2630 : Loss 3612.6089\n",
      "Iteration 2640 : Loss 3609.6890\n",
      "Iteration 2650 : Loss 3606.7870\n",
      "Iteration 2660 : Loss 3603.9026\n",
      "Iteration 2670 : Loss 3601.0358\n",
      "Iteration 2680 : Loss 3598.1863\n",
      "Iteration 2690 : Loss 3595.3540\n",
      "Iteration 2700 : Loss 3592.5388\n",
      "Iteration 2710 : Loss 3589.7405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2720 : Loss 3586.9590\n",
      "Iteration 2730 : Loss 3584.1942\n",
      "Iteration 2740 : Loss 3581.4459\n",
      "Iteration 2750 : Loss 3578.7139\n",
      "Iteration 2760 : Loss 3575.9982\n",
      "Iteration 2770 : Loss 3573.2986\n",
      "Iteration 2780 : Loss 3570.6149\n",
      "Iteration 2790 : Loss 3567.9471\n",
      "Iteration 2800 : Loss 3565.2950\n",
      "Iteration 2810 : Loss 3562.6584\n",
      "Iteration 2820 : Loss 3560.0373\n",
      "Iteration 2830 : Loss 3557.4315\n",
      "Iteration 2840 : Loss 3554.8409\n",
      "Iteration 2850 : Loss 3552.2654\n",
      "Iteration 2860 : Loss 3549.7048\n",
      "Iteration 2870 : Loss 3547.1591\n",
      "Iteration 2880 : Loss 3544.6280\n",
      "Iteration 2890 : Loss 3542.1115\n",
      "Iteration 2900 : Loss 3539.6095\n",
      "Iteration 2910 : Loss 3537.1218\n",
      "Iteration 2920 : Loss 3534.6483\n",
      "Iteration 2930 : Loss 3532.1890\n",
      "Iteration 2940 : Loss 3529.7436\n",
      "Iteration 2950 : Loss 3527.3122\n",
      "Iteration 2960 : Loss 3524.8945\n",
      "Iteration 2970 : Loss 3522.4905\n",
      "Iteration 2980 : Loss 3520.1000\n",
      "Iteration 2990 : Loss 3517.7229\n",
      "Iteration 3000 : Loss 3515.3592\n",
      "Iteration 3010 : Loss 3513.0088\n",
      "Iteration 3020 : Loss 3510.6714\n",
      "Iteration 3030 : Loss 3508.3471\n",
      "Iteration 3040 : Loss 3506.0357\n",
      "Iteration 3050 : Loss 3503.7370\n",
      "Iteration 3060 : Loss 3501.4511\n",
      "Iteration 3070 : Loss 3499.1779\n",
      "Iteration 3080 : Loss 3496.9171\n",
      "Iteration 3090 : Loss 3494.6687\n",
      "Iteration 3100 : Loss 3492.4326\n",
      "Iteration 3110 : Loss 3490.2088\n",
      "Iteration 3120 : Loss 3487.9971\n",
      "Iteration 3130 : Loss 3485.7974\n",
      "Iteration 3140 : Loss 3483.6096\n",
      "Iteration 3150 : Loss 3481.4337\n",
      "Iteration 3160 : Loss 3479.2695\n",
      "Iteration 3170 : Loss 3477.1170\n",
      "Iteration 3180 : Loss 3474.9760\n",
      "Iteration 3190 : Loss 3472.8465\n",
      "Iteration 3200 : Loss 3470.7284\n",
      "Iteration 3210 : Loss 3468.6216\n",
      "Iteration 3220 : Loss 3466.5260\n",
      "Iteration 3230 : Loss 3464.4415\n",
      "Iteration 3240 : Loss 3462.3681\n",
      "Iteration 3250 : Loss 3460.3056\n",
      "Iteration 3260 : Loss 3458.2540\n",
      "Iteration 3270 : Loss 3456.2132\n",
      "Iteration 3280 : Loss 3454.1831\n",
      "Iteration 3290 : Loss 3452.1636\n",
      "Iteration 3300 : Loss 3450.1546\n",
      "Iteration 3310 : Loss 3448.1561\n",
      "Iteration 3320 : Loss 3446.1680\n",
      "Iteration 3330 : Loss 3444.1902\n",
      "Iteration 3340 : Loss 3442.2226\n",
      "Iteration 3350 : Loss 3440.2652\n",
      "Iteration 3360 : Loss 3438.3179\n",
      "Iteration 3370 : Loss 3436.3805\n",
      "Iteration 3380 : Loss 3434.4531\n",
      "Iteration 3390 : Loss 3432.5355\n",
      "Iteration 3400 : Loss 3430.6277\n",
      "Iteration 3410 : Loss 3428.7296\n",
      "Iteration 3420 : Loss 3426.8411\n",
      "Iteration 3430 : Loss 3424.9622\n",
      "Iteration 3440 : Loss 3423.0928\n",
      "Iteration 3450 : Loss 3421.2328\n",
      "Iteration 3460 : Loss 3419.3821\n",
      "Iteration 3470 : Loss 3417.5407\n",
      "Iteration 3480 : Loss 3415.7086\n",
      "Iteration 3490 : Loss 3413.8856\n",
      "Iteration 3500 : Loss 3412.0716\n",
      "Iteration 3510 : Loss 3410.2667\n",
      "Iteration 3520 : Loss 3408.4707\n",
      "Iteration 3530 : Loss 3406.6836\n",
      "Iteration 3540 : Loss 3404.9053\n",
      "Iteration 3550 : Loss 3403.1358\n",
      "Iteration 3560 : Loss 3401.3750\n",
      "Iteration 3570 : Loss 3399.6228\n",
      "Iteration 3580 : Loss 3397.8791\n",
      "Iteration 3590 : Loss 3396.1440\n",
      "Iteration 3600 : Loss 3394.4173\n",
      "Iteration 3610 : Loss 3392.6990\n",
      "Iteration 3620 : Loss 3390.9890\n",
      "Iteration 3630 : Loss 3389.2872\n",
      "Iteration 3640 : Loss 3387.5937\n",
      "Iteration 3650 : Loss 3385.9083\n",
      "Iteration 3660 : Loss 3384.2311\n",
      "Iteration 3670 : Loss 3382.5618\n",
      "Iteration 3680 : Loss 3380.9005\n",
      "Iteration 3690 : Loss 3379.2472\n",
      "Iteration 3700 : Loss 3377.6017\n",
      "Iteration 3710 : Loss 3375.9640\n",
      "Iteration 3720 : Loss 3374.3340\n",
      "Iteration 3730 : Loss 3372.7118\n",
      "Iteration 3740 : Loss 3371.0972\n",
      "Iteration 3750 : Loss 3369.4902\n",
      "Iteration 3760 : Loss 3367.8907\n",
      "Iteration 3770 : Loss 3366.2988\n",
      "Iteration 3780 : Loss 3364.7142\n",
      "Iteration 3790 : Loss 3363.1371\n",
      "Iteration 3800 : Loss 3361.5673\n",
      "Iteration 3810 : Loss 3360.0047\n",
      "Iteration 3820 : Loss 3358.4495\n",
      "Iteration 3830 : Loss 3356.9013\n",
      "Iteration 3840 : Loss 3355.3604\n",
      "Iteration 3850 : Loss 3353.8265\n",
      "Iteration 3860 : Loss 3352.2996\n",
      "Iteration 3870 : Loss 3350.7798\n",
      "Iteration 3880 : Loss 3349.2669\n",
      "Iteration 3890 : Loss 3347.7609\n",
      "Iteration 3900 : Loss 3346.2617\n",
      "Iteration 3910 : Loss 3344.7694\n",
      "Iteration 3920 : Loss 3343.2838\n",
      "Iteration 3930 : Loss 3341.8049\n",
      "Iteration 3940 : Loss 3340.3327\n",
      "Iteration 3950 : Loss 3338.8671\n",
      "Iteration 3960 : Loss 3337.4081\n",
      "Iteration 3970 : Loss 3335.9556\n",
      "Iteration 3980 : Loss 3334.5097\n",
      "Iteration 3990 : Loss 3333.0702\n",
      "Iteration 4000 : Loss 3331.6371\n",
      "Iteration 4010 : Loss 3330.2103\n",
      "Iteration 4020 : Loss 3328.7899\n",
      "Iteration 4030 : Loss 3327.3758\n",
      "Iteration 4040 : Loss 3325.9679\n",
      "Iteration 4050 : Loss 3324.5662\n",
      "Iteration 4060 : Loss 3323.1707\n",
      "Iteration 4070 : Loss 3321.7813\n",
      "Iteration 4080 : Loss 3320.3980\n",
      "Iteration 4090 : Loss 3319.0207\n",
      "Iteration 4100 : Loss 3317.6495\n",
      "Iteration 4110 : Loss 3316.2842\n",
      "Iteration 4120 : Loss 3314.9248\n",
      "Iteration 4130 : Loss 3313.5713\n",
      "Iteration 4140 : Loss 3312.2237\n",
      "Iteration 4150 : Loss 3310.8819\n",
      "Iteration 4160 : Loss 3309.5459\n",
      "Iteration 4170 : Loss 3308.2156\n",
      "Iteration 4180 : Loss 3306.8910\n",
      "Iteration 4190 : Loss 3305.5721\n",
      "Iteration 4200 : Loss 3304.2589\n",
      "Iteration 4210 : Loss 3302.9512\n",
      "Iteration 4220 : Loss 3301.6491\n",
      "Iteration 4230 : Loss 3300.3525\n",
      "Iteration 4240 : Loss 3299.0614\n",
      "Iteration 4250 : Loss 3297.7758\n",
      "Iteration 4260 : Loss 3296.4956\n",
      "Iteration 4270 : Loss 3295.2208\n",
      "Iteration 4280 : Loss 3293.9513\n",
      "Iteration 4290 : Loss 3292.6872\n",
      "Iteration 4300 : Loss 3291.4284\n",
      "Iteration 4310 : Loss 3290.1749\n",
      "Iteration 4320 : Loss 3288.9265\n",
      "Iteration 4330 : Loss 3287.6834\n",
      "Iteration 4340 : Loss 3286.4454\n",
      "Iteration 4350 : Loss 3285.2126\n",
      "Iteration 4360 : Loss 3283.9849\n",
      "Iteration 4370 : Loss 3282.7622\n",
      "Iteration 4380 : Loss 3281.5446\n",
      "Iteration 4390 : Loss 3280.3320\n",
      "Iteration 4400 : Loss 3279.1244\n",
      "Iteration 4410 : Loss 3277.9217\n",
      "Iteration 4420 : Loss 3276.7240\n",
      "Iteration 4430 : Loss 3275.5311\n",
      "Iteration 4440 : Loss 3274.3431\n",
      "Iteration 4450 : Loss 3273.1600\n",
      "Iteration 4460 : Loss 3271.9817\n",
      "Iteration 4470 : Loss 3270.8081\n",
      "Iteration 4480 : Loss 3269.6393\n",
      "Iteration 4490 : Loss 3268.4752\n",
      "Iteration 4500 : Loss 3267.3158\n",
      "Iteration 4510 : Loss 3266.1610\n",
      "Iteration 4520 : Loss 3265.0109\n",
      "Iteration 4530 : Loss 3263.8655\n",
      "Iteration 4540 : Loss 3262.7246\n",
      "Iteration 4550 : Loss 3261.5882\n",
      "Iteration 4560 : Loss 3260.4564\n",
      "Iteration 4570 : Loss 3259.3291\n",
      "Iteration 4580 : Loss 3258.2063\n",
      "Iteration 4590 : Loss 3257.0880\n",
      "Iteration 4600 : Loss 3255.9740\n",
      "Iteration 4610 : Loss 3254.8645\n",
      "Iteration 4620 : Loss 3253.7593\n",
      "Iteration 4630 : Loss 3252.6586\n",
      "Iteration 4640 : Loss 3251.5621\n",
      "Iteration 4650 : Loss 3250.4699\n",
      "Iteration 4660 : Loss 3249.3820\n",
      "Iteration 4670 : Loss 3248.2984\n",
      "Iteration 4680 : Loss 3247.2190\n",
      "Iteration 4690 : Loss 3246.1438\n",
      "Iteration 4700 : Loss 3245.0729\n",
      "Iteration 4710 : Loss 3244.0060\n",
      "Iteration 4720 : Loss 3242.9433\n",
      "Iteration 4730 : Loss 3241.8847\n",
      "Iteration 4740 : Loss 3240.8302\n",
      "Iteration 4750 : Loss 3239.7798\n",
      "Iteration 4760 : Loss 3238.7335\n",
      "Iteration 4770 : Loss 3237.6911\n",
      "Iteration 4780 : Loss 3236.6528\n",
      "Iteration 4790 : Loss 3235.6184\n",
      "Iteration 4800 : Loss 3234.5880\n",
      "Iteration 4810 : Loss 3233.5615\n",
      "Iteration 4820 : Loss 3232.5390\n",
      "Iteration 4830 : Loss 3231.5203\n",
      "Iteration 4840 : Loss 3230.5056\n",
      "Iteration 4850 : Loss 3229.4946\n",
      "Iteration 4860 : Loss 3228.4876\n",
      "Iteration 4870 : Loss 3227.4843\n",
      "Iteration 4880 : Loss 3226.4848\n",
      "Iteration 4890 : Loss 3225.4891\n",
      "Iteration 4900 : Loss 3224.4971\n",
      "Iteration 4910 : Loss 3223.5089\n",
      "Iteration 4920 : Loss 3222.5243\n",
      "Iteration 4930 : Loss 3221.5435\n",
      "Iteration 4940 : Loss 3220.5663\n",
      "Iteration 4950 : Loss 3219.5928\n",
      "Iteration 4960 : Loss 3218.6230\n",
      "Iteration 4970 : Loss 3217.6567\n",
      "Iteration 4980 : Loss 3216.6940\n",
      "Iteration 4990 : Loss 3215.7349\n",
      "Iteration 5000 : Loss 3214.7794\n",
      "Iteration 5010 : Loss 3213.8274\n",
      "Iteration 5020 : Loss 3212.8789\n",
      "Iteration 5030 : Loss 3211.9340\n",
      "Iteration 5040 : Loss 3210.9925\n",
      "Iteration 5050 : Loss 3210.0545\n",
      "Iteration 5060 : Loss 3209.1199\n",
      "Iteration 5070 : Loss 3208.1888\n",
      "Iteration 5080 : Loss 3207.2610\n",
      "Iteration 5090 : Loss 3206.3367\n",
      "Iteration 5100 : Loss 3205.4158\n",
      "Iteration 5110 : Loss 3204.4982\n",
      "Iteration 5120 : Loss 3203.5839\n",
      "Iteration 5130 : Loss 3202.6730\n",
      "Iteration 5140 : Loss 3201.7654\n",
      "Iteration 5150 : Loss 3200.8611\n",
      "Iteration 5160 : Loss 3199.9600\n",
      "Iteration 5170 : Loss 3199.0623\n",
      "Iteration 5180 : Loss 3198.1677\n",
      "Iteration 5190 : Loss 3197.2764\n",
      "Iteration 5200 : Loss 3196.3883\n",
      "Iteration 5210 : Loss 3195.5034\n",
      "Iteration 5220 : Loss 3194.6217\n",
      "Iteration 5230 : Loss 3193.7432\n",
      "Iteration 5240 : Loss 3192.8677\n",
      "Iteration 5250 : Loss 3191.9955\n",
      "Iteration 5260 : Loss 3191.1263\n",
      "Iteration 5270 : Loss 3190.2603\n",
      "Iteration 5280 : Loss 3189.3973\n",
      "Iteration 5290 : Loss 3188.5374\n",
      "Iteration 5300 : Loss 3187.6806\n",
      "Iteration 5310 : Loss 3186.8268\n",
      "Iteration 5320 : Loss 3185.9760\n",
      "Iteration 5330 : Loss 3185.1283\n",
      "Iteration 5340 : Loss 3184.2835\n",
      "Iteration 5350 : Loss 3183.4418\n",
      "Iteration 5360 : Loss 3182.6030\n",
      "Iteration 5370 : Loss 3181.7671\n",
      "Iteration 5380 : Loss 3180.9342\n",
      "Iteration 5390 : Loss 3180.1042\n",
      "Iteration 5400 : Loss 3179.2772\n",
      "Iteration 5410 : Loss 3178.4530\n",
      "Iteration 5420 : Loss 3177.6317\n",
      "Iteration 5430 : Loss 3176.8133\n",
      "Iteration 5440 : Loss 3175.9978\n",
      "Iteration 5450 : Loss 3175.1851\n",
      "Iteration 5460 : Loss 3174.3752\n",
      "Iteration 5470 : Loss 3173.5681\n",
      "Iteration 5480 : Loss 3172.7639\n",
      "Iteration 5490 : Loss 3171.9624\n",
      "Iteration 5500 : Loss 3171.1637\n",
      "Iteration 5510 : Loss 3170.3678\n",
      "Iteration 5520 : Loss 3169.5747\n",
      "Iteration 5530 : Loss 3168.7842\n",
      "Iteration 5540 : Loss 3167.9965\n",
      "Iteration 5550 : Loss 3167.2116\n",
      "Iteration 5560 : Loss 3166.4293\n",
      "Iteration 5570 : Loss 3165.6497\n",
      "Iteration 5580 : Loss 3164.8728\n",
      "Iteration 5590 : Loss 3164.0985\n",
      "Iteration 5600 : Loss 3163.3269\n",
      "Iteration 5610 : Loss 3162.5580\n",
      "Iteration 5620 : Loss 3161.7917\n",
      "Iteration 5630 : Loss 3161.0280\n",
      "Iteration 5640 : Loss 3160.2669\n",
      "Iteration 5650 : Loss 3159.5084\n",
      "Iteration 5660 : Loss 3158.7524\n",
      "Iteration 5670 : Loss 3157.9991\n",
      "Iteration 5680 : Loss 3157.2483\n",
      "Iteration 5690 : Loss 3156.5000\n",
      "Iteration 5700 : Loss 3155.7543\n",
      "Iteration 5710 : Loss 3155.0111\n",
      "Iteration 5720 : Loss 3154.2704\n",
      "Iteration 5730 : Loss 3153.5322\n",
      "Iteration 5740 : Loss 3152.7965\n",
      "Iteration 5750 : Loss 3152.0633\n",
      "Iteration 5760 : Loss 3151.3326\n",
      "Iteration 5770 : Loss 3150.6043\n",
      "Iteration 5780 : Loss 3149.8785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5790 : Loss 3149.1551\n",
      "Iteration 5800 : Loss 3148.4341\n",
      "Iteration 5810 : Loss 3147.7155\n",
      "Iteration 5820 : Loss 3146.9994\n",
      "Iteration 5830 : Loss 3146.2856\n",
      "Iteration 5840 : Loss 3145.5742\n",
      "Iteration 5850 : Loss 3144.8652\n",
      "Iteration 5860 : Loss 3144.1586\n",
      "Iteration 5870 : Loss 3143.4543\n",
      "Iteration 5880 : Loss 3142.7523\n",
      "Iteration 5890 : Loss 3142.0527\n",
      "Iteration 5900 : Loss 3141.3554\n",
      "Iteration 5910 : Loss 3140.6604\n",
      "Iteration 5920 : Loss 3139.9677\n",
      "Iteration 5930 : Loss 3139.2773\n",
      "Iteration 5940 : Loss 3138.5892\n",
      "Iteration 5950 : Loss 3137.9034\n",
      "Iteration 5960 : Loss 3137.2198\n",
      "Iteration 5970 : Loss 3136.5385\n",
      "Iteration 5980 : Loss 3135.8594\n",
      "Iteration 5990 : Loss 3135.1826\n",
      "Iteration 6000 : Loss 3134.5079\n",
      "Iteration 6010 : Loss 3133.8355\n",
      "Iteration 6020 : Loss 3133.1653\n",
      "Iteration 6030 : Loss 3132.4973\n",
      "Iteration 6040 : Loss 3131.8315\n",
      "Iteration 6050 : Loss 3131.1679\n",
      "Iteration 6060 : Loss 3130.5064\n",
      "Iteration 6070 : Loss 3129.8471\n",
      "Iteration 6080 : Loss 3129.1900\n",
      "Iteration 6090 : Loss 3128.5350\n",
      "Iteration 6100 : Loss 3127.8821\n",
      "Iteration 6110 : Loss 3127.2313\n",
      "Iteration 6120 : Loss 3126.5827\n",
      "Iteration 6130 : Loss 3125.9361\n",
      "Iteration 6140 : Loss 3125.2917\n",
      "Iteration 6150 : Loss 3124.6494\n",
      "Iteration 6160 : Loss 3124.0091\n",
      "Iteration 6170 : Loss 3123.3709\n",
      "Iteration 6180 : Loss 3122.7348\n",
      "Iteration 6190 : Loss 3122.1007\n",
      "Iteration 6200 : Loss 3121.4687\n",
      "Iteration 6210 : Loss 3120.8387\n",
      "Iteration 6220 : Loss 3120.2108\n",
      "Iteration 6230 : Loss 3119.5848\n",
      "Iteration 6240 : Loss 3118.9609\n",
      "Iteration 6250 : Loss 3118.3390\n",
      "Iteration 6260 : Loss 3117.7191\n",
      "Iteration 6270 : Loss 3117.1012\n",
      "Iteration 6280 : Loss 3116.4852\n",
      "Iteration 6290 : Loss 3115.8713\n",
      "Iteration 6300 : Loss 3115.2593\n",
      "Iteration 6310 : Loss 3114.6492\n",
      "Iteration 6320 : Loss 3114.0411\n",
      "Iteration 6330 : Loss 3113.4350\n",
      "Iteration 6340 : Loss 3112.8308\n",
      "Iteration 6350 : Loss 3112.2285\n",
      "Iteration 6360 : Loss 3111.6281\n",
      "Iteration 6370 : Loss 3111.0297\n",
      "Iteration 6380 : Loss 3110.4331\n",
      "Iteration 6390 : Loss 3109.8385\n",
      "Iteration 6400 : Loss 3109.2457\n",
      "Iteration 6410 : Loss 3108.6548\n",
      "Iteration 6420 : Loss 3108.0658\n",
      "Iteration 6430 : Loss 3107.4787\n",
      "Iteration 6440 : Loss 3106.8934\n",
      "Iteration 6450 : Loss 3106.3100\n",
      "Iteration 6460 : Loss 3105.7284\n",
      "Iteration 6470 : Loss 3105.1486\n",
      "Iteration 6480 : Loss 3104.5707\n",
      "Iteration 6490 : Loss 3103.9946\n",
      "Iteration 6500 : Loss 3103.4204\n",
      "Iteration 6510 : Loss 3102.8479\n",
      "Iteration 6520 : Loss 3102.2772\n",
      "Iteration 6530 : Loss 3101.7084\n",
      "Iteration 6540 : Loss 3101.1413\n",
      "Iteration 6550 : Loss 3100.5760\n",
      "Iteration 6560 : Loss 3100.0125\n",
      "Iteration 6570 : Loss 3099.4507\n",
      "Iteration 6580 : Loss 3098.8908\n",
      "Iteration 6590 : Loss 3098.3325\n",
      "Iteration 6600 : Loss 3097.7760\n",
      "Iteration 6610 : Loss 3097.2213\n",
      "Iteration 6620 : Loss 3096.6683\n",
      "Iteration 6630 : Loss 3096.1170\n",
      "Iteration 6640 : Loss 3095.5675\n",
      "Iteration 6650 : Loss 3095.0196\n",
      "Iteration 6660 : Loss 3094.4735\n",
      "Iteration 6670 : Loss 3093.9290\n",
      "Iteration 6680 : Loss 3093.3863\n",
      "Iteration 6690 : Loss 3092.8453\n",
      "Iteration 6700 : Loss 3092.3059\n",
      "Iteration 6710 : Loss 3091.7682\n",
      "Iteration 6720 : Loss 3091.2322\n",
      "Iteration 6730 : Loss 3090.6978\n",
      "Iteration 6740 : Loss 3090.1652\n",
      "Iteration 6750 : Loss 3089.6341\n",
      "Iteration 6760 : Loss 3089.1047\n",
      "Iteration 6770 : Loss 3088.5770\n",
      "Iteration 6780 : Loss 3088.0508\n",
      "Iteration 6790 : Loss 3087.5263\n",
      "Iteration 6800 : Loss 3087.0035\n",
      "Iteration 6810 : Loss 3086.4822\n",
      "Iteration 6820 : Loss 3085.9625\n",
      "Iteration 6830 : Loss 3085.4445\n",
      "Iteration 6840 : Loss 3084.9280\n",
      "Iteration 6850 : Loss 3084.4132\n",
      "Iteration 6860 : Loss 3083.8999\n",
      "Iteration 6870 : Loss 3083.3882\n",
      "Iteration 6880 : Loss 3082.8781\n",
      "Iteration 6890 : Loss 3082.3695\n",
      "Iteration 6900 : Loss 3081.8626\n",
      "Iteration 6910 : Loss 3081.3571\n",
      "Iteration 6920 : Loss 3080.8532\n",
      "Iteration 6930 : Loss 3080.3509\n",
      "Iteration 6940 : Loss 3079.8501\n",
      "Iteration 6950 : Loss 3079.3508\n",
      "Iteration 6960 : Loss 3078.8531\n",
      "Iteration 6970 : Loss 3078.3569\n",
      "Iteration 6980 : Loss 3077.8622\n",
      "Iteration 6990 : Loss 3077.3690\n",
      "Iteration 7000 : Loss 3076.8773\n",
      "Iteration 7010 : Loss 3076.3872\n",
      "Iteration 7020 : Loss 3075.8985\n",
      "Iteration 7030 : Loss 3075.4113\n",
      "Iteration 7040 : Loss 3074.9256\n",
      "Iteration 7050 : Loss 3074.4414\n",
      "Iteration 7060 : Loss 3073.9586\n",
      "Iteration 7070 : Loss 3073.4773\n",
      "Iteration 7080 : Loss 3072.9975\n",
      "Iteration 7090 : Loss 3072.5192\n",
      "Iteration 7100 : Loss 3072.0422\n",
      "Iteration 7110 : Loss 3071.5668\n",
      "Iteration 7120 : Loss 3071.0928\n",
      "Iteration 7130 : Loss 3070.6202\n",
      "Iteration 7140 : Loss 3070.1490\n",
      "Iteration 7150 : Loss 3069.6793\n",
      "Iteration 7160 : Loss 3069.2110\n",
      "Iteration 7170 : Loss 3068.7442\n",
      "Iteration 7180 : Loss 3068.2787\n",
      "Iteration 7190 : Loss 3067.8146\n",
      "Iteration 7200 : Loss 3067.3520\n",
      "Iteration 7210 : Loss 3066.8907\n",
      "Iteration 7220 : Loss 3066.4308\n",
      "Iteration 7230 : Loss 3065.9724\n",
      "Iteration 7240 : Loss 3065.5153\n",
      "Iteration 7250 : Loss 3065.0595\n",
      "Iteration 7260 : Loss 3064.6052\n",
      "Iteration 7270 : Loss 3064.1522\n",
      "Iteration 7280 : Loss 3063.7006\n",
      "Iteration 7290 : Loss 3063.2504\n",
      "Iteration 7300 : Loss 3062.8015\n",
      "Iteration 7310 : Loss 3062.3539\n",
      "Iteration 7320 : Loss 3061.9077\n",
      "Iteration 7330 : Loss 3061.4628\n",
      "Iteration 7340 : Loss 3061.0193\n",
      "Iteration 7350 : Loss 3060.5771\n",
      "Iteration 7360 : Loss 3060.1362\n",
      "Iteration 7370 : Loss 3059.6967\n",
      "Iteration 7380 : Loss 3059.2584\n",
      "Iteration 7390 : Loss 3058.8215\n",
      "Iteration 7400 : Loss 3058.3859\n",
      "Iteration 7410 : Loss 3057.9516\n",
      "Iteration 7420 : Loss 3057.5185\n",
      "Iteration 7430 : Loss 3057.0868\n",
      "Iteration 7440 : Loss 3056.6564\n",
      "Iteration 7450 : Loss 3056.2272\n",
      "Iteration 7460 : Loss 3055.7994\n",
      "Iteration 7470 : Loss 3055.3728\n",
      "Iteration 7480 : Loss 3054.9474\n",
      "Iteration 7490 : Loss 3054.5234\n",
      "Iteration 7500 : Loss 3054.1006\n",
      "Iteration 7510 : Loss 3053.6791\n",
      "Iteration 7520 : Loss 3053.2588\n",
      "Iteration 7530 : Loss 3052.8398\n",
      "Iteration 7540 : Loss 3052.4220\n",
      "Iteration 7550 : Loss 3052.0055\n",
      "Iteration 7560 : Loss 3051.5902\n",
      "Iteration 7570 : Loss 3051.1761\n",
      "Iteration 7580 : Loss 3050.7633\n",
      "Iteration 7590 : Loss 3050.3516\n",
      "Iteration 7600 : Loss 3049.9413\n",
      "Iteration 7610 : Loss 3049.5321\n",
      "Iteration 7620 : Loss 3049.1241\n",
      "Iteration 7630 : Loss 3048.7174\n",
      "Iteration 7640 : Loss 3048.3118\n",
      "Iteration 7650 : Loss 3047.9075\n",
      "Iteration 7660 : Loss 3047.5043\n",
      "Iteration 7670 : Loss 3047.1024\n",
      "Iteration 7680 : Loss 3046.7016\n",
      "Iteration 7690 : Loss 3046.3020\n",
      "Iteration 7700 : Loss 3045.9036\n",
      "Iteration 7710 : Loss 3045.5064\n",
      "Iteration 7720 : Loss 3045.1103\n",
      "Iteration 7730 : Loss 3044.7154\n",
      "Iteration 7740 : Loss 3044.3217\n",
      "Iteration 7750 : Loss 3043.9291\n",
      "Iteration 7760 : Loss 3043.5377\n",
      "Iteration 7770 : Loss 3043.1475\n",
      "Iteration 7780 : Loss 3042.7584\n",
      "Iteration 7790 : Loss 3042.3704\n",
      "Iteration 7800 : Loss 3041.9836\n",
      "Iteration 7810 : Loss 3041.5979\n",
      "Iteration 7820 : Loss 3041.2133\n",
      "Iteration 7830 : Loss 3040.8299\n",
      "Iteration 7840 : Loss 3040.4476\n",
      "Iteration 7850 : Loss 3040.0665\n",
      "Iteration 7860 : Loss 3039.6864\n",
      "Iteration 7870 : Loss 3039.3075\n",
      "Iteration 7880 : Loss 3038.9297\n",
      "Iteration 7890 : Loss 3038.5529\n",
      "Iteration 7900 : Loss 3038.1773\n",
      "Iteration 7910 : Loss 3037.8028\n",
      "Iteration 7920 : Loss 3037.4294\n",
      "Iteration 7930 : Loss 3037.0571\n",
      "Iteration 7940 : Loss 3036.6859\n",
      "Iteration 7950 : Loss 3036.3157\n",
      "Iteration 7960 : Loss 3035.9467\n",
      "Iteration 7970 : Loss 3035.5787\n",
      "Iteration 7980 : Loss 3035.2118\n",
      "Iteration 7990 : Loss 3034.8459\n",
      "Iteration 8000 : Loss 3034.4812\n",
      "Iteration 8010 : Loss 3034.1175\n",
      "Iteration 8020 : Loss 3033.7548\n",
      "Iteration 8030 : Loss 3033.3932\n",
      "Iteration 8040 : Loss 3033.0327\n",
      "Iteration 8050 : Loss 3032.6732\n",
      "Iteration 8060 : Loss 3032.3148\n",
      "Iteration 8070 : Loss 3031.9574\n",
      "Iteration 8080 : Loss 3031.6011\n",
      "Iteration 8090 : Loss 3031.2457\n",
      "Iteration 8100 : Loss 3030.8915\n",
      "Iteration 8110 : Loss 3030.5382\n",
      "Iteration 8120 : Loss 3030.1860\n",
      "Iteration 8130 : Loss 3029.8348\n",
      "Iteration 8140 : Loss 3029.4846\n",
      "Iteration 8150 : Loss 3029.1355\n",
      "Iteration 8160 : Loss 3028.7873\n",
      "Iteration 8170 : Loss 3028.4402\n",
      "Iteration 8180 : Loss 3028.0941\n",
      "Iteration 8190 : Loss 3027.7490\n",
      "Iteration 8200 : Loss 3027.4048\n",
      "Iteration 8210 : Loss 3027.0617\n",
      "Iteration 8220 : Loss 3026.7196\n",
      "Iteration 8230 : Loss 3026.3785\n",
      "Iteration 8240 : Loss 3026.0383\n",
      "Iteration 8250 : Loss 3025.6991\n",
      "Iteration 8260 : Loss 3025.3610\n",
      "Iteration 8270 : Loss 3025.0238\n",
      "Iteration 8280 : Loss 3024.6875\n",
      "Iteration 8290 : Loss 3024.3523\n",
      "Iteration 8300 : Loss 3024.0180\n",
      "Iteration 8310 : Loss 3023.6847\n",
      "Iteration 8320 : Loss 3023.3523\n",
      "Iteration 8330 : Loss 3023.0209\n",
      "Iteration 8340 : Loss 3022.6905\n",
      "Iteration 8350 : Loss 3022.3610\n",
      "Iteration 8360 : Loss 3022.0324\n",
      "Iteration 8370 : Loss 3021.7049\n",
      "Iteration 8380 : Loss 3021.3782\n",
      "Iteration 8390 : Loss 3021.0525\n",
      "Iteration 8400 : Loss 3020.7277\n",
      "Iteration 8410 : Loss 3020.4039\n",
      "Iteration 8420 : Loss 3020.0810\n",
      "Iteration 8430 : Loss 3019.7590\n",
      "Iteration 8440 : Loss 3019.4380\n",
      "Iteration 8450 : Loss 3019.1179\n",
      "Iteration 8460 : Loss 3018.7987\n",
      "Iteration 8470 : Loss 3018.4804\n",
      "Iteration 8480 : Loss 3018.1630\n",
      "Iteration 8490 : Loss 3017.8466\n",
      "Iteration 8500 : Loss 3017.5311\n",
      "Iteration 8510 : Loss 3017.2164\n",
      "Iteration 8520 : Loss 3016.9027\n",
      "Iteration 8530 : Loss 3016.5898\n",
      "Iteration 8540 : Loss 3016.2779\n",
      "Iteration 8550 : Loss 3015.9669\n",
      "Iteration 8560 : Loss 3015.6567\n",
      "Iteration 8570 : Loss 3015.3475\n",
      "Iteration 8580 : Loss 3015.0391\n",
      "Iteration 8590 : Loss 3014.7316\n",
      "Iteration 8600 : Loss 3014.4250\n",
      "Iteration 8610 : Loss 3014.1193\n",
      "Iteration 8620 : Loss 3013.8144\n",
      "Iteration 8630 : Loss 3013.5104\n",
      "Iteration 8640 : Loss 3013.2073\n",
      "Iteration 8650 : Loss 3012.9051\n",
      "Iteration 8660 : Loss 3012.6037\n",
      "Iteration 8670 : Loss 3012.3031\n",
      "Iteration 8680 : Loss 3012.0035\n",
      "Iteration 8690 : Loss 3011.7047\n",
      "Iteration 8700 : Loss 3011.4067\n",
      "Iteration 8710 : Loss 3011.1096\n",
      "Iteration 8720 : Loss 3010.8134\n",
      "Iteration 8730 : Loss 3010.5180\n",
      "Iteration 8740 : Loss 3010.2234\n",
      "Iteration 8750 : Loss 3009.9297\n",
      "Iteration 8760 : Loss 3009.6368\n",
      "Iteration 8770 : Loss 3009.3447\n",
      "Iteration 8780 : Loss 3009.0535\n",
      "Iteration 8790 : Loss 3008.7631\n",
      "Iteration 8800 : Loss 3008.4735\n",
      "Iteration 8810 : Loss 3008.1848\n",
      "Iteration 8820 : Loss 3007.8969\n",
      "Iteration 8830 : Loss 3007.6098\n",
      "Iteration 8840 : Loss 3007.3235\n",
      "Iteration 8850 : Loss 3007.0380\n",
      "Iteration 8860 : Loss 3006.7534\n",
      "Iteration 8870 : Loss 3006.4695\n",
      "Iteration 8880 : Loss 3006.1865\n",
      "Iteration 8890 : Loss 3005.9042\n",
      "Iteration 8900 : Loss 3005.6228\n",
      "Iteration 8910 : Loss 3005.3422\n",
      "Iteration 8920 : Loss 3005.0623\n",
      "Iteration 8930 : Loss 3004.7833\n",
      "Iteration 8940 : Loss 3004.5050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8950 : Loss 3004.2275\n",
      "Iteration 8960 : Loss 3003.9509\n",
      "Iteration 8970 : Loss 3003.6750\n",
      "Iteration 8980 : Loss 3003.3999\n",
      "Iteration 8990 : Loss 3003.1255\n",
      "Iteration 9000 : Loss 3002.8520\n",
      "Iteration 9010 : Loss 3002.5792\n",
      "Iteration 9020 : Loss 3002.3072\n",
      "Iteration 9030 : Loss 3002.0359\n",
      "Iteration 9040 : Loss 3001.7655\n",
      "Iteration 9050 : Loss 3001.4958\n",
      "Iteration 9060 : Loss 3001.2268\n",
      "Iteration 9070 : Loss 3000.9586\n",
      "Iteration 9080 : Loss 3000.6912\n",
      "Iteration 9090 : Loss 3000.4245\n",
      "Iteration 9100 : Loss 3000.1586\n"
     ]
    }
   ],
   "source": [
    "# (9) 모델 학습하기\n",
    "\n",
    "#y_pred = model(X_train, W, b)\n",
    "\n",
    "\n",
    "losses = []\n",
    "MAX_ITER = 10**5\n",
    "for i in range(1, MAX_ITER):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L)\n",
    "\n",
    "    # iteration마다 loss값 확인하기\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))\n",
    "\n",
    "    # 특정 loss값보다 작아지면 학습을 종료한다.\n",
    "    if losses[-1] < 3000:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66b236ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2875.4950042072533"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (10) test 데이터에 대한 성능 확인하기\n",
    "prediction = model(X_test, W, b)\n",
    "mse = loss(X_test, W, b, y_test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b4208c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 실제 정답과 모델 예측값을 잠깐 비교해보자.\n",
    "# print(\"정답 | 모델예측값\")\n",
    "# for idx in range(len(y_test)):\n",
    "#     print(y_test[idx], \"|\", prediction[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5254cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA2NklEQVR4nO2deXxV5Z3wvz9CAsEFEHBhUWjLgAwgIFjb0EV5kbZuaBVtq9U6ykyt2mVeMI4WeX07r7F06jbV6mhHnal1cClSbV0qWoXWymbRUhfUIAmyKrgAJiS/9497EpKbc5Nzc/Zzf9/PJ59773OW+9xzTp7f8/xWUVUMwzAMA6BH3B0wDMMwkoMJBcMwDKMVEwqGYRhGKyYUDMMwjFZMKBiGYRitmFAwDMMwWjGhYBh5iMjPReSHHvd9RkQuDLtPhhEVJhSMkkNEakVkt4h8ICI7ROSPIvJPItIDQFX/SVX/bwT9MIFiJA4TCkapcrKqHgAcAdQAlwN3xtslw4gfEwpGSaOqO1V1MXAWcJ6IjBWRu0TkRwAi0l9EHhGRrSLynvN+aN5pPikiL4jI+yLysIgc1LJBRI51ViI7ROQvIvJFp/1fgc8B/y4iH4rIvzvto0XkSRF5V0ReFZFZbc71FRFZ66xw6kXkf4d6cYySxISCYQCq+gJQR26gbksP4D/JrSgOB3YD/563zzeBC4DDgL3ATQAiMgR4FPgRcBDwv4EHRWSQql4JPAdcoqr7q+olIrIf8CRwL3AwcDZwi4iMcb7nTuAfnRXOWGBJQD/fMFoxoWAY+9hIbvBuRVW3q+qDqrpLVT8A/hX4Qt5x/6WqL6vqR8APgVkiUgacA/xWVX+rqs2q+iSwAvhKge8/CahV1f9U1b2quhp4EDjT2d4IjBGRA1X1PVVdFcSPNoy2mFAwjH0MAd5t2yAifUTkNhFZLyLvA88C/ZxBv4UNbd6vB8qBgeRWF2c6qqMdIrIDmEpuReHGEcCn8/b/BnCos/2r5ATKehH5g4h8xs+PNQw3esbdAcNIAiIyhZxQWAp8us2mfwZGAZ9W1U0iMgFYDUibfYa1eX84uRn9NnLC4r9U9aICX5ufongD8AdVne66s+py4FQRKQcuARbmfbdh+MZWCkZJIyIHishJwH3Af6vqS3m7HEDOjrDDMSBf7XKac0RkjIj0Aa4BHlDVJuC/gZNFZIaIlIlIbxH5YhtD9WbgE23O8wjwdyJyroiUO39TRORIEakQkW+ISF9VbQTeB5oDuxCG4WBCwShVfiMiH5CbnV8J/BT4lst+NwCV5Gb+zwOPuezzX8BdwCagN3AZgKpuAE4F/gXY6nzXHPb9390InOF4Nd3k2CxOIGdg3uic7zqgl7P/uUCto8b6J3KqJcMIFLEiO4ZhGEYLtlIwDMMwWjGhYBiGYbRiQsEwDMNoxYSCYRiG0Uqq4xQGDhyow4cPj7sbhmEYqWLlypXbVHWQ27ZUC4Xhw4ezYsWKuLthGIaRKkRkfaFtpj4yDMMwWjGhYBiGYbRiQsEwDMNoJdU2BTcaGxupq6tjz549cXclM/Tu3ZuhQ4dSXl4ed1cMwwiZzAmFuro6DjjgAIYPH46IdH2A0Smqyvbt26mrq2PEiBFxd8cwjJDJnFDYs2dPyQiE93Y1sHnnHhqamqko68EhfXvTv09FoN8hIgwYMICtW7cGel7DMJJJ5oQCUDICof693TQ7CQ0bmpqpf283QCiCwTCM0sAMzSll8849rQKhhWZVNu80W4phGN3HhEJM1NbWcu+993b7+FtuWODa3tBkdVcMw+g+JhRiwq9QuPPfr3dtryizW2oYRvcp+RFk0ep6qmqWMKL6UapqlrBodb2v882bN48bbrih9fOVV17JjTfe2GG/6upqnnvuOSZMmMD1119PU1MTc+bMYcqUKYwfP57bbrsNgHfeeYfPf/7zTJgwgbFjx/Lcc89RXV3Nnj27mTXjc1xx6b7yvz1EOKRvb1/9NwyjtEl15bXJkydrfu6jv/3tbxx55JGejl+0up4rHnqJ3Y1NrW2V5WVce/o4Zk4c0q0+1dbWcvrpp7Nq1Sqam5sZOXIkL7zwAgMGDGi33zPPPMNPfvITHnnkEQBuv/12tmzZwlVXXcXHH39MVVUV999/Pw899BB79uzhyiuvpKmpiV27dnHAAQew//77s2HLu6F7H7VQzHU1DCPZiMhKVZ3sti007yMR6Q08S66+bE9yxcyvFpER5IqkDwBWAueqaoOI9ALuAY4GtgNnqWptWP0DWPD4q+0EAsDuxiYWPP5qt4XC8OHDGTBgAKtXr2bz5s1MnDixg0Bw44knnmDNmjU88MADAOzcuZPXX3+dKVOmcMEFF9DY2MjMmTOZMGFC6zH9+1SEJgQMwyhNwnRJ/Rg4XlU/FJFyYKmI/A74AXC9qt4nIj8H/gG41Xl9T1U/JSJnkytYflaI/WPjjt1FtXvlwgsv5K677mLTpk1ccMEFno5RVW6++WZmzJjRYduzzz7Lo48+yvnnn88PfvADvvnNb/rqn2EYRiFCsylojg+dj+XOnwLHAw847XcDM533pzqfcbZPk5Ad5Af3qyyq3SunnXYajz32GMuXL3cd5AEOOOAAPvjgg9bPM2bM4NZbb6WxsRGA1157jY8++oj169dzyCGHcNFFF3HhhReyatUqAMrLy1v3NQzDCIpQg9dEpIyciuhTwM+AN4AdqrrX2aUOaNHTDAE2AKjqXhHZSU7FtC3vnLOB2QCHH364r/7NmTHK1aYwZ8YoX+etqKjguOOOo1+/fpSVlbnuM378eMrKyjjqqKM4//zz+e53v0ttbS2TJk1CVRk0aBCLFi3imWeeYcGCBZSXl7P//vtzzz33ADB79mzGjx/PpEmT+OUvf+mrv0b6WLS6ngWPv8rGHbsZ3K+SOTNGdVvlaRhticTQLCL9gF8DPwTuUtVPOe3DgN+p6lgReRn4kqrWOdveAD6tqtsKnNa3oRnC+edqbm5m0qRJ3H///YwcOdLXuZKCGZqTQxgOEsV8twmj9BOLobktqrpDRJ4GPgP0E5GezmphKNDiA1oPDAPqRKQn0JecwTlUZk4cEuhDvXbtWk466SROO+20zAgEI1mE4SDhhXxhVL9jN1c89BKACYYMEab30SCg0REIlcB0csbjp4EzyHkgnQc87Byy2Pn8J2f7Ek2hv+yYMWN48803Wz+/9NJLnHvuue326dWrF3/+85+j7pqREcJykOiKuISRES1hrhQOA+527Ao9gIWq+oiIrAXuE5EfAauBO5397wT+S0TWAe8CZ4fYt8gYN24cL774YtzdMDLE4H6V1LsIAL8OEl0RlzAyoiU0oaCqa4CJLu1vAse4tO8BzgyrP4aRFcJykOiKuISRES0ln+bCMNLGzIlDuPb0cQzpV4kAQ/pVRmJknjNjFJXl7b3pohBGRrRksp6CYWSdoB0kvH4nYN5HGceEgmEYnolDGBnRYuqjhPPMM89w0kknAbB48WJqamoK7rtjxw5uueWW1s8bN27kjDPOCL2PhmFkBxMKMdHU1NT1TnmccsopVFdXF9yeLxQGDx7cmmDPMAzDCyYU1iyE68fC/H651zULfZ+ytraW0aNH841vfIMjjzySM844g127djF8+HAuv/zy1mjnJ554gs985jNMmjSJM888kw8/zKWKeuyxxxg9ejSTJk3ioYceaj3vXXfdxSWXXALA5s2bOe200zjqqKM46qij+OMf/0h1dTVvvPEGEyZMYM6cOdTW1jJ27FgA9uzZw7e+9S3GjRvHxIkTefrpp1vPefrpp/OlL32JkSNHMnfuXN+/3zD8EnSdk9gIYXwJm9K2KaxZCL+5DBodN7udG3KfAcbP8nXqV199lTvvvJOqqiouuOCC1hn8gAEDWLVqFdu2beP000/n97//Pfvttx/XXXcdP/3pT5k7dy4XXXQRS5Ys4VOf+hRnneWeKPayyy7jC1/4Ar/+9a9pamriww8/pKamhpdffrk1LqK2trZ1/5/97GeICC+99BKvvPIKJ5xwAq+99hoAL774IqtXr6ZXr16MGjWKSy+9lGHDhvn6/YbRXTITOR3i+BImpb1SeOqafTeshcbduXafDBs2jKqqKgDOOeccli5dCtA6yD///POsXbuWqqoqJkyYwN1338369et55ZVXGDFiBCNHjkREOOecc1zPv2TJEr797W8DUFZWRt++fTvtz9KlS1vPNXr0aI444ohWoTBt2jT69u1L7969GTNmDOvXr/f9+w2ju3QWOZ0qQhxfwqS0Vwo764prL4L8rN8tn/fbbz8gVz9h+vTp/OpXv2q3XxzRz7169Wp9X1ZWxt69ezvZ2zDCJTOR0yGOL2FS2iuFvkOLay+Ct99+mz/96U8A3HvvvUydOrXd9mOPPZZly5axbt06AD766CNee+01Ro8eTW1tLW+88QZAB6HRwrRp07j11luBnNF6586dHWo0tOVzn/tca4rt1157jbfffptRoyzoyEgeYdU5iZwQx5cwKW2hMG0elOc9aOWVuXafjBo1ip/97GcceeSRvPfee62qnhYGDRrEXXfdxde+9jXGjx/PZz7zGV555RV69+7N7bffzoknnsikSZM4+OCDXc9/44038vTTTzNu3DiOPvpo1q5dy4ABA6iqqmLs2LHMmTOn3f4XX3wxzc3NjBs3jrPOOou77rqr3QrBMIKmu8bizEROhzi+hEkk9RTCIoh6CqxZmNPx7azLSfBp83wbgWpraznppJN4+eWXfZ0nSVg9BaMY/NZ8yEzdhhDGlyCIvZ5Cohk/KxE3yeg+mRlAMoTfNNuZiZxO4fhiQiEEhg8fnqlVQpLJjPtixsiMsbgEyaRNIc0qsSSS5OuZGffFjJEZY3EJkjmh0Lt3b7Zv357ogSxNqCrbt2+nd+/ecXfFFZuRJpPMGItLkMypj4YOHUpdXR1bt26NuyuZoXfv3gwdmkw3Oiv8kkwszXZ6yZxQKC8vZ8SIEXF3w4iIuKqQGV2TGWNxiZE5oWCUFjYjLZ60emultd9pw4SCkXpsRuqdtHprpbXfaSRzhmbDMAqTVm+ttPY7jZhQMIwSIq3eWmntdxox9VGJYvrZ0iSt3lpp7XcasZVCCdKin63fsRtln342tdWtDM+kNX4grf1OIyYUShDTz5YuMycO4drTxzGkXyUCDOlX6TlJXZyktd9pxNRHJYjpZ0ubtHprpbXfacOEQgli+tnoMRuOkRZMfVSCmH42WsyGY6SJ0ISCiAwTkadFZK2I/FVEvuu0zxeRehF50fn7SptjrhCRdSLyqojMCKtvpY7pZ6PFbDhGmghTfbQX+GdVXSUiBwArReRJZ9v1qvqTtjuLyBjgbODvgcHA70Xk71S1/X+TEQimn40Os+HEj6nvvBPaSkFV31HVVc77D4C/AZ3dhVOB+1T1Y1V9C1gHHBNW/wwjKqy2QLyY+q44IrEpiMhwYCLwZ6fpEhFZIyK/EJH+TtsQYEObw+pwESIiMltEVojICkuPbaQBs+HEi6nviiN0oSAi+wMPAt9T1feBW4FPAhOAd4B/K+Z8qnq7qk5W1cmDBg0KuruGkSu2fv1YmN8v97pmoa/TmQ0nILp5X0x9VxyhuqSKSDk5gfBLVX0IQFU3t9n+H8Ajzsd6YFibw4c6bYYRHWsWsvfhS+nZtCf3eeeG3GfwVYDdbDg+WbMQfnMZNDoD+c4Nuc/Q5X0xF+ziCNP7SIA7gb+p6k/btB/WZrfTgJYK94uBs0Wkl4iMAEYCL4TVP8NwY9fv5u0TCA49m/aw63fzYuqRAcBT1+wTCC007s61d4Gp74ojzJVCFXAu8JKIvOi0/QvwNRGZAChQC/wjgKr+VUQWAmvJeS59xzyPjKjpvXtTUe1GROysK669DVaIqThCEwqquhQQl02/7eSYfwX+Naw+GUZXbGwewNAe29zbY+hPybJmYW4VsLMO+g6Fyv6w+92O+/X1dldMfecdi2g2jDbcUXEOu7SiXdsureCOinNi6lHyWbS6nqqaJYyofpSqmiX+XT1b7Ac7NwCae234EHqUt9+vvBKmmVovaEwoGEYbJpw4m3k6m7rmgTSrUNc8kHk6mwknzo67a4kklBgAN/tBUwP0OgD6DgMk93ryTb6M/4Y7lhDPMNqQUzFczFmPTzP9swc6iwHo9jUrZCfY/R5c/lb3zml4xoRCArGQ/HiJU/+ctnsfSgxA36GO6sil3QgdUx8lDAvJL13SeO9DSeExbR57y3q3a9pb1tvsBxFhQiFhWEh+6ZLGex9GDMCipiqqGy9sZ9epbryQRU1VfrsbOYEb4SPA1EcJw0LyS5c03vswYgAWPP4q9Q2f5QE+2679T37sFDHQsvJrEfQtKz8g0b/DhELCsJD80iWt9z5oG0wahaMboRjhI8DURwnDQvJLF7v3ObKSajytws2EQsKwjJqli937HFkRjmkVbqKqcfeh20yePFlXrFgRdzcMwwiYtLnmupFvU4CccEuCoBeRlao62W2b2RQMwwdZGLyKIj8n0bR5oUQVZyFXUVoT8ZlQMAKj1AbItHqXdBsfNQ1KlTQKNxMKRiCU3ABJer1Luk2Bmga7fjeP6b8dWDKTgaxjhmYjENIYeOWXtHqXdJsCOYl679qUqihso3NMKBiBUHIDJOn1Luk2BXIPbdQB7T5nfTKQdUwoGIFQcgMk2XGd9My0ebkaBm3YpRX8eG9He0KWJwNZx4SCEQiFBsjjRg9KXe4Xr5RcXMH4WbkaBm1qGvy4/GIWN0/tsGuWJwNZxwzNRiC4ud8dN3oQD66sz7TxOY3eJb4YP6udp9GE1fVUuvjiZ3a1VAKYUDACI3+ArKpZUlreOVESUbxAV3Tqi5+QPhrFYULBCI1SND5HQsLiBVxXSwnro+EdsykYoVGKxudIKBAvwFPXxNMfN9LQR8MVEwpGaJScd05UFKphXKg9DtLQR8MVEwpGaJScd05UFKpVnKQaxmnoo+GK2RSMUCk575womDavvb4ecvEDSaphnIY+Gq6YUMgYpZaUriRpMdQm2bMnhD7asx0NVk8hQyQ5f7th+MGe7WDprJ5CaDYFERkmIk+LyFoR+auIfNdpP0hEnhSR153X/k67iMhNIrJORNaIyKSw+uaHRavrExuhW4pJ6YzSwJ7t6AjT0LwX+GdVHQMcC3xHRMYA1cBTqjoSeMr5DPBlYKTzNxu4NcS+dYuW2UpSM0KWRFzAmoVw/ViY3y/3umah9acEKIlnOyGEJhRU9R1VXeW8/wD4GzAEOBW429ntbmCm8/5U4B7N8TzQT0QOC6t/3SHps5XMxwW0BETt3ADovoCoR34Qz8BcqD8mGAIn8892gojEJVVEhgMTgT8Dh6jqO86mTcAhzvshwIY2h9U5bfnnmi0iK0RkxdatW8PrtAtJn61kPi6gUEDUil/EMzBnPUArQaugzD/bCSJ0oSAi+wMPAt9T1ffbbtOclbsoS7eq3q6qk1V18qBBgwLsadckfbaS+biAgoFPeY9QVANzlgO0ErYKyvyznSBCdUkVkXJyAuGXqvqQ07xZRA5T1Xcc9dAWp70eGNbm8KFOW2KYM2OUqwdEkmYrmY4L6DvUGaQ8EMXAXKg/WQjQ6mwVFJPra6af7QQRpveRAHcCf1PVn7bZtBg4z3l/HvBwm/ZvOl5IxwI726iZEoHNVrogbHWDS5EXEPd9oxiY3frjN0ArKSqbrK2CknJdU0CYK4Uq4FzgJRF50Wn7F6AGWCgi/wCsB1qmHb8FvgKsA3YB3wqxb90mUbOVgFMT+woOiiIrpltA1MgT4C/3xhM5G3SAVpIyi2ZpFZSk65oCLHgtiXgZ7PMfdMgNhiff1K0H3Xdw0PVjCwwiw+D7Lxfdn6LISt7+OK9hPgE/X936/qDuaZKua0LoLHjN0lwkDa+zGp863/xVwa6Gvf4K4sSpbsirBpZakqSyiTOVRtAz+yRd1xRgQiFpeB3sfTzo+auC+k5caj272xZSN1T2d2ZqCVBxJZ2kqWziErZBG7mTdl0TjgmFpOF1sC/iQc8fSD/6uOOqoBCe3W3dsmKWVcDHH8Dud53f4G/G5ybMMlXzOarMoklXtwU9s7eMrUVhQiFpeB3sPT7oxawK8inK3dZN3dDw0T6B0EIxM768wevFj77K7sZj2u2yu7GJFx+9nZnPPJjcQc4rYahs8gVAvmE+iUbXoGf2acgqmyDM0Jw0ijHweZjxVdUs8SwI+lWWs1+vnsGpZub3wz02UWD+js6PdbkOu7SC6sYLWdw8tbXtlB5LqSm/gz7SsO/YKA2iScbtWUJwvSdJMrrGbeQuAczQnCaKmdV40Pl6tQlUlpcx/5S/dxUC3dbj+5nxueiV+0gDc3suZHHDPqEwt+fC9gIBYg+ySgxuuvlCCQSSZHTN0Mw+jTYwEwpJJEAD3+B+la4rhf59yulT0fWqwJce348ut8AgNVi2d/q5q+NLimKuQdKMrhnwKEurDcxqNGecQonErj7571lWfTxv1ZzIsurjCz6kvjLDjp+VW/L3HQZI7tWrCqDAILWnz6HtIsr39Dm0qONLioLXIC8K3IyuoZD0rMqFsJVCxmkZ7Lu7hN24Yzen9FjK3J4LGSzb2KgD+fHeWfxmx9SuD4buz/gKrDL6fPkalo0/fl/bmmvMs6QQ0+bBw9+BpjbqtbIKmHguvP5E6lUzSSfpWZULYUIhLfhwI/STmuO8/V9gbuM+Q+5Q2UZN+R0cVF4BnNitc3rCq145Q/rnUMh3JFGFw4+Fk37qvn9bku66mnAG96vk6Pef7DChWnng9Li71inmfZQGYvTG2HXdaPrs7piXcFflYfS5/JVQv9vwiZ/0DuYB5Jvli29j7MqrqGzjCLFbK3j56B8x5ZR/jLFnMdVoNgIkxmIubgKhs3YjQfgJAktJAaEk10yf8sbN7QQCQKU0MOWNm2PqkTe6VB+JyKXAf6vqexH0x3AjztwtUgbqEv0sZR3bjGThxyW44DO3IdC0JX5InHdPvrqtUO2PhHvGeVkpHAIsF5GFIvIlp06CESWF/okj8LBRN4HQSXuQJHkWmAr81HvozHMpIdXYEuXd41apLs5aHz7oUiio6lXASHIFc84HXheR/ycinwy5b0YLYRRz8chm3EueFmoPipZZYP2O3Sj7ZoFXLXrJBIVX/LgEFyxoFFPpUxcS5d1TMFAwePffsCdLnmwKTi3lTc7fXqA/8ICI/DjQ3hju+Pnn9sm1DWeySyvate3SCq5tODPU7y00C/zl8293EBQmGDph/KycUXn+jtyr12fG7ZlLWDR0omqmd1Y/PMD/20KTpSD/B7zYFL4LfBPYBtwBzFHVRhHpAbwOzA2sN0ZhYorwXHHgdKrfx3Gr285GHRCJW12h2V7+sFRUzQejOPKfuYLeTPGoQxJVM72g/SbYnFKdqcyC+h/wEqdwEHC6qq5v26iqzSJyUiC9MBJL7h+voV2+ocryMq4N+R+vUHoON5IeDBQngebeGXkCrLizY/tBn4jF+Ow3MDNQIkrPHYXKrEuhoKpXd7Ltb4H1xEgkcf3juc0CC+T3jEddkAIC9855/Qn39reepfXORJyKe2bZMmb2ugZ610GvoVA2j31l3yMkoiDKKALiLHjNSCz5s9zjRg/iwZX13a8jXWIUSps+pF8ly6qPdzmiCwqmQnehkNokyCjpEgywCyogzlJnG6nELT3H5CMOSoa6IAUErmrozPc+HzfDa9C1l4Mu25kCprxxMxQMiAsmStqEQpGkMT96lvCTxylxhJxbqJBdptvqNje9ecGiPS7G56AH8TiDOuMigt9saS6KIAp3MKNEcAt2CjgQrFDa9G5757i5qU6+wHsMTdADWoxBnbERwW82oVAEiYqgNNJNBLmFZk4cwrWnj2tXf8Kv/WV57Xts2rmHZlU27dzD8uZR3mNogh7QYgzqjI0IfrOpj4ogURGURrqJSPURpLqtnZFT4FC20nflVSznR0zx4osftNtmKaZNj+A3m1AogsB1tEYm8WR38pOsLiaGrVrgmvVz2KoF4MXzJYwBLQNlO4sm5N9sQqEIEhVBaSQSz7EBEQU7+SFfuD2nW11zvB2s27yftBQH8ZQRmk1BRH4hIltE5OU2bfNFpF5EXnT+vtJm2xUisk5EXhWRGWH1yw9h6GgjYc3CXMTp/H6515iyWpYCnu1OMeaz8oKbU8VGHei67/uyvz1fGSK04DUR+TzwIXCPqo512uYDH6rqT/L2HQP8CjgGGAz8Hvg77SI/swWveaBQgM/QY6B2aa5WgpTB0ed7K9FodMqI6kddw7sEeKsmxPKlAeMW+HZKj6XUlO8rzQrQoD0p6wFlunffjmEFkFl50MCIpfKaqj4LvOtx91OB+1T1Y1V9C1hHTkAYfink5fLWH/YVz9GmXE6bR34Qff8yRiH70nn7v5Cq2bSb88Ti5qlUN17IJgbRrMImBqEV+7UXCBBOOu0IXHiNHHG4pF4iImsc9VJ/p20I0NbqVue0dUBEZovIChFZsXXr1rD7mn6K8WZZeVdo3QiMhKvC3GIDzqj4I1fpz1M1oBUSbisPnM6h89fR4//s4ND56+jV+L77CYIOIEtJedAsELVQuBX4JDABeAf4t2JPoKq3q+pkVZ08aFC4hV4yQTHeLBFUU/NFCmaLbnana/Z7kJ5Ne9rvmPABzXPgW1QBZKUYvRwTkQoFVd2sqk2q2gz8B/tURPXAsDa7DnXaDL+4VtAqQNLrLqdktjhz4hCWVR/PWzUnsqz6ePrs3uS+Y4IHNM9OFVEFkJVi9HJMROqSKiKHqeo7zsfTgBbPpMXAvSLyU3KG5pHAC1H2LTO4GeNOvql920GfyNkU8jn6/Mi7WxRpnS1W9ofdLua1yv4d2xKEp8C3qALIUuDCmxVCEwoi8ivgi8BAEakDrga+KCITyGXQqsVJ66eqfxWRhcBacuU+v9OV51F3yXRCu0JZKE++qX0a4zULYf1SaG5ziXuUweHHRtvfYklhwFfcRPK8RxF7UIrRyzFRUvUU8gOLIGP5+AuWS8zLbe91v6SR1vz5BesQSK52ckhk/nk3uk0sLqlJJPMJ7byqVwrlxPeaKz8ukhjw5cUbKiZ9eOafdyMUSirNReYT2nlVr0iZu6dRnIZmr4FJSUqT4LVozLR57H340nYeSHvLetMzZH145p93IxRKaqVQyPc6MwntvHqCFDLXxOWSGqOr6aLV9VTVLGFE9aNU1SwprjaGR2+oRU1VVDdeSF3zQJpVqGseSHXjhSxqqgrgFxQm88+7EQolJRQCLzqSNLyqV/oOcz28YHvYxORqWqho0vLFt3kLkPOorlvw+Ks80PBZpjbcxCc+/iVTG27igYbPuqtxAgzOy/zzboRCSamPWoxrmfU+Am/qlaS598Xkauqmc5/e9AfGrroT+NjpQyd1hD26mnpW4wRcw7gknncjcEpKKEDGavx2l6S598Xkauo2WM/tuZDKFoHQgs9i8J7rcIRQiN6ed6NYSkp9ZOxjUVMVVR/fxIg9v6Tq45vC0297UYfEVFbRTbc+WArUBnBbtex+z33fvHbPapy0BucZmcKEQglSSJdelJHVC2sWsvfhS9sZkPc+fGlHwRCTq6nbYP0O7jUDXFctHl1NPaeMsFQORgIoqeA1I4dbrnzIDVbLqo8P7Ht2XTeaPrvf6dheeRh9Ln8lsO/xQ37E7w1jXmfKS1d7C5ALOpiu0PmO+jq8/kQyVH1GJugseK3kbApGdP7rvQskgivUHgcdde7Hw/D+3mMmIDjbjNv5Rp4Af7k3MOOzYXSFCYUSpJDhM1cI5vLAZqQbmwcwtEdHHf3G5gEkWiFSTIBc0MF0+ee7fqw/47NVKzOKxIRCCTJnxiiW/voWvsd9DJZtbNSB/IGJnKXPwU4n6jaAGekdFecwt/GWduUbd2kFd1Scw3yfv6EtmU5y6Mf4HLCLq1EamKG5BJlZtoya8jsY2mMbPQSG9tjG13s8GXghmAknzmaezm4XyTtPZzPhxNk+f8E+IjOax4Uf43NK6k8YycJWCqXIU9d0EABSaF8f7pC52frFnPX4tNBm8Z0lfcvEamHaPFh0MTQ37mvrUe7NXddcXI1uYEKhFClmUPDpDhl28FRJJH0T6fxzIUIICkycqs5sJoFj6qNSpOCgkDfYpKCyVeaTvj11DTQ1tG9ravCmAgo4KDBxqroU1OxOIyYUoiTAZGe+KDRYTL4gWbUKPJDIpG9B3mc/KqCAgwITV5/BbCahYOqjqEiSJ0jSch8VQ566YOa0eXB6VXJUGkHfZ78qoABdZhOnqjObSSiYUCAiPWkIyc58kaRiNV4pMODOPPkmZlZ387cErZMO+j4Xymg78gSnrGp0Qt1zYr+osJrdoVDy6qPI9KQ2q/FP0OqCMHTSPu9zh6I/TVUdVUBHfT0X5RyxLj1xqrqYEilmnZIXCpHpSS3ZmX+CFqxh6KR93OeCE5SmKvj+yzB/R+719Sdi0aV7TuzXCb4q3eWTxJrdGaDk1UeR6UmTVtgmjQStLghj9VbMfc5TXb340VfZ3XhMu11cYy5iXHX6cTFuEXotk7AWoddy3m6RRjVowin5lUJkLo02q/FP0OqCMFZvXu+zi+pqbuMtnNJjaYdT1u/Y3W52vavy0OD7HQFhrMoDXXkYQCmuFPJmZzeMuZRvLj+i3cMamp7UZjX+KOQ1Bd0zuoa1evNyn11UV32kgbk9F7K4YWq7doFWA2/9jt3Mq/gqNeV3tI9KT8GqM+hVeSgrD6PEVgous7MpL13NPVPW+9KTGhEyflZ7/Tp031gc5+qtgKpnsGxv91mA/IonDzR8lh/JP3W/3zHFywS9Kk9c3ERGKK2VQgHD4pQ3bmZZ9cvx9Mnwh18X0LhWbwXsI3v6HMqQyspW92g3F1CAuz88hvlX/Z/ivzfGeJk5M0a1m9mDv1V54uImMkJprRTMLTR7pPWeFrCP9PnyNSyrPp63ak5kWfXxDAna5hVjFHAQ3kttyXyKk5gIbaUgIr8ATgK2qOpYp+0g4H+A4UAtMEtV3xMRAW4EvgLsAs5X1VWBdyrFwS6JS0TmlbATlqX1nnqMKj9u9CD++/m3Oxx+3OhB3fvemIVokAkSg155GDnCXCncBXwpr60aeEpVRwJPOZ8BvgyMdP5mA7eG0qMIg12C9IpIXCIyr0SRsCzNAUz59hEXYfn0K1tdDy3U3iUZipcJeuVh5AhtpaCqz4rI8LzmU4EvOu/vBp4BLnfa71FVBZ4XkX4icpiqdqz67oegvVcKELRXRGprBkSR2iPNeZxcyF8RFrIpdFtvnrF4mbBTs5ciURuaD2kz0G8CDnHeDwHa6gDqnLYOQkFEZpNbTXD44YcX34N8w2IQhrfuBiF5JAyDml91lKfjo1JVZMTV120y4eZ9BD705lEKUat1kEpi8z5SVRURt+e9q+NuB24HmDx5ctHHd8DvbNZFqMzVW3i3RwOLm9v7m3d3EA86EZnflYzn40uhyEuAuK0IlY5uqb715lEI0SRlBTaKImrvo80ichiA87rFaa8HhrXZb6jTFj5+Z7OdBCHl091BPOhEZH79uz0fP20ee8t6t2vaW9Y7O0VeAqbQpEEhfXpzq3WQWqJeKSwGzgNqnNeH27RfIiL3AZ8GdgZuTyiEz9ms7qxzrW+cH4TkZxBvGQCCmiH7VUd5PX5RUxVLGy/ke9zHYNnORh3ADc1nM7WpiplF9ThHam0rHim0IhzSr5Jl1cfH0CMfpNVV2AjVJfVX5IzKA0WkDrianDBYKCL/AKwHWtaRvyXnjrqOnEvqt8LqVwd8Gt42M5BD6egJskkGMKRfZWBqjiANan7VUV6PX/D4q9Q3fJYH+Gy79j8lyLYSJ/mqsONGD2LPqvscIbqNjTqQGzibqTMujrurxZNWV2EjVO+jrxXYNM1lXwW+E1ZfOsWn4e3ahjO5tvwO+si+Orq7tILrGmexbH4yZ3d+/bu9Hh/0IF6MMIvK9rB88W0MW7WAg3UrW2QQGybNYcop/9jlcW52mT2r7uP/lf0HFfoxAENlGzVld9Cz7Cj2zZ/ixfN1jcjLKcs2prgorTQXhfBheFtx4HSq34e5PRe2qkh+vHcWKw+cHnAng8OvOsrr8UEbyL0Ko6gSpS1ffBtjV15FpTSAwKFspe/Kq1gOXQoGN1XY97ivVSC00LNpT3zV+fIo6rpG4OVkCfHCQXKT9HQyefJkXbFiRax9yH8wITdQpcIYGDJhXBsvM8OqmiWR6OY3zf+Uu+qQQRw6f12nx46ofrSDq+mbvb5ODzcDFZILcAuI7s6uo7quXqmqWcLR7z/pTMhy6raWCVnqbDARIyIrVXWy2zZbKRRL0gvHJ4igDeQt5+zq+KhsDwfrVty8DA7WbV0e67aK2qgDGSouxwaoh/czu06aTWfy+0+2U90OlW3UlN/BFe8DmFDoLiYUiiGMwvEZJ46I06gKzG+RQa4rhS0ykAJlcFqZM2MUcx74C41N+9YL/9Z8FgvK7wy1ToIfD66orqtXrqi4nz40tGvrIw1cUXE/cG0sfcoCpZUl1S/me108MeTuj6rA/LOHf5tdWtGubZdW8Ozh3/Z2gjz90SPNU1l91DWh1nfwM9uP6rp65RDcV2SF2g1v2EqhGMz3ujhiimoNQ23lxo1bJrK08cKOTgZbJnbpK7Tg8VdpbG4vFRqble+tHRlqbQ8/s/2orqtXpIDbq5jbqy9MKBSD+V4XRxQJ8QoQhdpq447d1DO1Y/lMD7PuuPTzft2RE5WALmPJ/ZKCqY+KIc1pmuMg4ysrP0Ve4ioQk6l003GWU80wtlIohoylaQ6djK+s/My64ywQ43W2n4rAsIxkyE0SJhSKxR5C74SxvE9QOmY/Ovak6efzscCw0sWC14xwCXIQzzdcQ07ImMogcJIWqGYEiwWvGfER5MoqRsN1qZG0QDUjOkwoGNHT3dVDAQO17qxjas2SrtUwCVI9+SVsfX/SAtWM6DDvIyNaWlRAOzcAui92wUNQ265K9zjhjTqg68I7Pr43aURRbChpgWppYNHqeqpqljCi+lGqapaktviTCYVi8RGhm5WHxhc+osJ/t+co8k1gqvD7pgnt2lyrwGUoGr2YynndfeYy5boaAVmqCmjqo2IoJkI3T1Wx/JOXcsXyI8ybw4cK6NNNK5C8aYwInNTjeaZVvNguU6a8D1x/2T5VkZtrbCf9STKeK9/59CBKVKBawslSVUBbKRSD19mmi6pi7KofMr3pD+12K6YucmYoEKPgRQU0uMd212MPkg8Z2mMbPQSG9tjGT8pvZ0HFbe1VRa5FUwv3J8kM7lfJKT2WsrTiMt7s9XWWVlzGKT2Wula+81OL2/BOlgzzJhSKwWuErovwqORj5vbsqGpK40PjC5eo8N304rrG9istt8FrTwGbguSN9xWylwqa8vZSOgiGlEaj3zDmda4rv6OdILyu/A5uGPN6u/3iHqhKSV0aV4R6GJhQKIZCs8r89gLCY7B0nOmm8aHxhUtqguqGf2Bx89QOu+YPXn2+fA17y3q3aysuykYzkRJhyhs356q9taFSGpjyxs3t2uIcqLKkY/dClgzzZlMoBq8RugV02O8woN3ntD40vsmLXVhRswS8uD+On5V7YNvYahp2fUCvxh3evrfvMPh+eBlIo0J31rkqw/Lb40ylkSUduxeSHqFeDCYUisFr7qMCwmPjuLkMWVuZiIfGt5+7D5///O8+bvQgHlxZ723wyhMo1/7oaubqLa3VtwAatCeItlchpVRV5MZmBroW98m17yPOgSpu1VUcZMUwb0KhWLxE6BYQHlPGz2LZKeF3sSt857XxUSfB7bsfXFnPV48ewtOvbC168Lr7w2N4t0dDh5oGAtw46DeZCFTL59qGM9uVoYRccZ9rG8/kxrx94xqoLPgtvZhQCIsEJ87zvbT3kW6i0Hc//crWbuXUGdyvksU7OtY0GNKvEr6fzZKMKw6cTvX7dCzuc+D0uLvWSpyqK8MfJhRKkEJL+MnvP9net7/Q7NpHnYSg1QqlOPjkfnNDO0FYWV7GtQn6zVnSsZcaJhRKELel/Sk9llJTcSfs/DjX0JlKyEedhKDVCqU4+KTlN2dFx15qWOrsEiRfrw+wrNdlDBGXguduHjs+Uli7fXdleZmlUDCMCLHU2UY73Gaag/e4Rwu7qoSKqEDn5uV07enjYpnlpqKSmGHETCwrBRGpBT4AmoC9qjpZRA4C/gcYDtQCs1T1vc7OYyuFALl+bAGVUPd9+5O0KkhSXwwjbjpbKcQZ0Xycqk5o07Fq4ClVHQk85Xw2osIl/YRf3/4k5d5JUl8MI8kkSX10KvBF5/3dwDPA5XF1puQYP4vlte8xbNUCDtZtbJGBbBg3hyk+3GqTFMAUVl9MJWVkjbiEggJPiIgCt6nq7cAhqvqOs30TcEhMfStJFq2ud1J77wt/qlxexrXD6rs9yCUpgCmMvlhxeyOLxKU+mqqqk4AvA98Rkc+33ag5Q4ersUNEZovIChFZsXVrx1B/o3uEoV5JUpKwMPpiKikji8QiFFS13nndAvwaOAbYLCKHATivWwoce7uqTlbVyYMGDYqqy5knDPVKkqp3hdGXJKnHDCMoIlcfich+QA9V/cB5fwJwDbAYOA+ocV4fjrpvpUxYqp4kBTB57YtXO0GS1GOGERRxrBQOAZaKyF+AF4BHVfUxcsJguoi8Dvwv57MREUlS9cRJMXUA7JoZWSTylYKqvgkc5dK+HZgWdX+MHGlJnRA2xSQLtGtmZJEkuaQaMZMkVU9cFGsnsGtmZA0TCkaqCDsuwOwERqljNZqN1BBF3V+zExiljgkFIzVEEReQJDdaw4gDUx8ZqSGquACzExiljK0UjNRQSK9v+n7DCA4TCkZqMH2/YYSPqY+M1GBxAYYRPiYUjFRh+n7DCBdTHxmGYRitmFAwDMMwWjGhYBiGYbRiQsEwDMNoxYSCYRiG0YrkKl+mExHZCqyP4KsGAtsi+J40YdfEHbsu7th1cSeu63KEqrqWrky1UIgKEVmhqpPj7keSsGvijl0Xd+y6uJPE62LqI8MwDKMVEwqGYRhGKyYUvHF73B1IIHZN3LHr4o5dF3cSd13MpmAYhmG0YisFwzAMoxUTCoZhGEYrJhQAETlIRJ4Ukded1/4F9ntMRHaIyCN57SNE5M8isk5E/kdEKqLpebgUcV3Oc/Z5XUTOa9P+jIi8KiIvOn8HR9f74BGRLzm/Z52IVLts7+Xc/3XO8zC8zbYrnPZXRWRGpB0Pme5eFxEZLiK72zwfP4+88yHh4Zp8XkRWicheETkjb5vr/1NkqGrJ/wE/Bqqd99XAdQX2mwacDDyS174QONt5/3Pg23H/pqiuC3AQ8Kbz2t9539/Z9gwwOe7fEdC1KAPeAD4BVAB/Acbk7XMx8HPn/dnA/zjvxzj79wJGOOcpi/s3JeC6DAdejvs3xHRNhgPjgXuAM9q0F/x/iurPVgo5TgXudt7fDcx020lVnwI+aNsmIgIcDzzQ1fEpxMt1mQE8qarvqup7wJPAl6LpXqQcA6xT1TdVtQG4j9z1aUvb6/UAMM15Pk4F7lPVj1X1LWCdc74s4Oe6ZJUur4mq1qrqGqA579jY/59MKOQ4RFXfcd5vAg4p4tgBwA5V3et8rgOyUgXGy3UZAmxo8zn/9/+noxr4YcoHgq5+Z7t9nOdhJ7nnw8uxacXPdQEYISKrReQPIvK5sDsbEX7ud+zPSslUXhOR3wOHumy6su0HVVURKRk/3ZCvyzdUtV5EDgAeBM4lt1w2DIB3gMNVdbuIHA0sEpG/V9X34+5YKVMyQkFV/1ehbSKyWUQOU9V3ROQwYEsRp94O9BORns4saChQ77O7kRHAdakHvtjm81BytgRUtd55/UBE7iW3rE6rUKgHhrX57HafW/apE5GeQF9yz4eXY9NKt6+L5pToHwOo6koReQP4O2BF6L0OFz/3u+D/U1SY+ijHYqDFyn8e8LDXA50H+2mgxYOgqOMTjpfr8jhwgoj0d7yTTgAeF5GeIjIQQETKgZOAlyPoc1gsB0Y6nmYV5Aymi/P2aXu9zgCWOM/HYuBsxwtnBDASeCGifodNt6+LiAwSkTIAEfkEuevyZkT9DhMv16QQrv9PIfXTnbgt9Un4I6fffAp4Hfg9cJDTPhm4o81+zwFbgd3kdH0znPZPkPsnXwfcD/SK+zdFfF0ucH77OuBbTtt+wEpgDfBX4EZS7nEDfAV4jZxnyZVO2zXAKc773s79X+c8D59oc+yVznGvAl+O+7ck4boAX3WejReBVcDJcf+WCK/JFGcM+YjcavKvbY7t8P8U5Z+luTAMwzBaMfWRYRiG0YoJBcMwDKMVEwqGYRhGKyYUDMMwjFZMKBiGYRitmFAwjIAQkWEi8paIHOR87u98Hh5z1wzDMyYUDCMgVHUDcCtQ4zTVALeram1snTKMIrE4BcMIECd6eyXwC+AiYIKqNsbbK8PwTsnkPjKMKFDVRhGZAzwGnGACwUgbpj4yjOD5MrkMoGPj7ohhFIsJBcMIEBGZAEwHjgW+72SXNYzUYELBMALCKSJ0K/A9VX0bWAD8JN5eGUZxmFAwjOC4CHhbVZ90Pt8CHCkiX4ixT4ZRFOZ9ZBiGYbRiKwXDMAyjFRMKhmEYRismFAzDMIxWTCgYhmEYrZhQMAzDMFoxoWAYhmG0YkLBMAzDaOX/A3iZMPlGVs3xAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (11) 정답 데이터와 예측한 데이터 시각화하기\n",
    "plt.scatter(X_test[:, 0], y_test)\n",
    "plt.scatter(X_test[:, 0], prediction)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Diabetes\")\n",
    "plt.legend([\"y_test\", \"prediction\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714d37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c0a8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b3cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
