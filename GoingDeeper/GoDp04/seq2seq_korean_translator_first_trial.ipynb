{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0093e01a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한글 폰트 세팅 완료\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "print(\"한글 폰트 세팅 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6becd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import gzip\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ddbb9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 188247\n",
      "Example:\n",
      ">> korean-english-park.train.en\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000000644 \u0000000765 \u0000000024 \u000000055552615 12360160402 016155\u0000 0\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000ustar\u000000pj\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000staff\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000000000 \u0000000000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000Much of personal computing is about \"can you top this?\"\n",
      ">> Amid mounting pressure on North Korea to abandon its nuclear weapons program Japanese and North Korean diplomats have resumed talks on normalizing diplomatic relations.\n",
      ">> “Guard robots are used privately and professionally to detect intruders or fire,” Karlsson said.\n",
      ">> Authorities from the Water Resources Ministry plan to begin construction next year on the controversial and hugely expensive project.\n",
      ">> Researchers also have debated whether weight-training has a big impact on the heart, since it does not give the heart and lungs the kind of workout they get from aerobic activities such as brisk walking or running for at least 20 minutes.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 살펴보기\n",
    "file_dir = os.getcwd() + '/korean-english-park.train.tar.gz'\n",
    "\n",
    "with gzip.open(file_dir, 'rt') as f:\n",
    "    raw = f.read().splitlines()\n",
    "    \n",
    "print(\"Data Size:\", len(raw))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775f39d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['korean-english-park.train.en\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00000644 \\x00000765 \\x00000024 \\x0000055552615 12360160402 016155\\x00 0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00ustar\\x0000pj\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00staff\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00000000 \\x00000000 \\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Much of personal computing is about \"can you top this?\"', 'so a mention a few weeks ago about a rechargeable wireless optical mouse brought in another rechargeable, wireless mouse.', \"Like all optical mice, But it also doesn't need a desk.\"]\n",
      "['\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00korean-english-park.train.ko\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00000644 \\x00000765 \\x00000024 \\x0000064302104 12360160402 016146\\x00 0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00ustar\\x0000pj\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00staff\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00000000 \\x00000000 \\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"', '모든 광마우스와 마찬가지 로 이 광마우스도 책상 위에 놓는 마우스 패드를 필요로 하지 않는다.', '그러나 이것은 또한 책상도 필요로 하지 않는다.']\n"
     ]
    }
   ],
   "source": [
    "mid = len(raw) // 2 # 한국어와 영어 텍스트의 경계\n",
    "print(raw[:3])\n",
    "print(raw[mid: mid + 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3889a6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "딕셔너리 크기: 77591\n"
     ]
    }
   ],
   "source": [
    "# 리스트를 한국어 문장인 키와 영어 문장인 값으로 분리\n",
    "# 데이터의 형태: ['영어', '영어', ... '한국어', '한국어']\n",
    "\n",
    "keys =  raw[mid:]  # 뒤 쪽 절반인 한국어 부분  \n",
    "values = raw[:mid] # 앞 쪽 절반인 영어부분\n",
    "\n",
    "# zip() 함수와 dict() 함수를 사용하여 딕셔너리 생성\n",
    "dictionary = dict(zip(keys, values))\n",
    "\n",
    "print(\"딕셔너리 크기:\",len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd5b450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74832"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 딕셔너리 내에 값의 중복을 제거하는 함수\n",
    "\n",
    "def remove_duplicate_values(original_dict):\n",
    "    # 값을 키로, 키를 값으로 하는 임시 딕셔너리 생성\n",
    "    temp_dict = {val: key for key, val in original_dict.items()}\n",
    "    # 다시 원래 형태로 변환하여 중복 제거된 딕셔너리 생성\n",
    "    result_dict = {val: key for key, val in temp_dict.items()}\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "unique_dict = remove_duplicate_values(dictionary)\n",
    "len(unique_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "461662c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('약 200600밀리그램의, 비슷한 분량의 카페인은 또한 육체적 지구력을 강화시키는 데 효과적인 것 같으며, 특히 고도가 높은 곳에서 약해진 육체적 지구력을 일부 회복시켜주는 데 유용하다는 것이 이번 연구에서 밝혀졌다.', 'A similar dose of caffeine, about 200-600 mg, also appears effective in enhancing physical endurance and may be especially useful in returning some of the physical endurance lost at high altitude, the study found.')\n",
      "\n",
      "('의약 연구소는 정부에 과학 문제에 관해 자문하기 위해 의회가 설립 인가를 내어 준 민간 단체인 국립 과학 학회의 부속 단체이다.', 'The Institute of Medicine is part of the National Academy of Sciences, a private organization chartered by Congress to advise the government on scientific matters.')\n",
      "\n",
      "('아시아에서의 왕성한 수요 덕분에 일본의 수출이 9월에 연속 6개월간 증가하여, 무역 흑자가 1조550억엔(84억7,000만달러)으로 늘어났다고 재무성이 발표했다.', 'thanks to robust demand in Asia, boosting the trade surplus to 1.055 trillion yen ($8.47 billion), the Ministry of Finance said.')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 딕셔너리 매핑 확인하기\n",
    "print(list(unique_dict.items())[10], end='\\n\\n')\n",
    "print(list(unique_dict.items())[11], end='\\n\\n')\n",
    "print(list(unique_dict.items())[12], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f87fb68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74832"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus = list(unique_dict.items())\n",
    "len(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e4ed272",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 텍스트 길이: 74832\n",
      "영어 텍스트 길이: 74832\n",
      "\n",
      "중국이 604억달러의 비용이 소요되는 아마도 세계 에서 가장 값비싼 계획 중의 하나이며, 중국 역사상 가장 규모가 큰 수로 변경 계획을 밝혔다.\n",
      "China has unveiled plans for the largest water-diversion in its history and possibly one of the world's most expensive at $60.4 billion.\n",
      "\n",
      "이 사업은 중국에서 가장 긴 강인 양쯔강의 물을 강 유역이 말라 들어가고 있는 북부의 황허, 화이허, 그리고 하이허, 세 개의 강으로 돌리게 될 것이다.\n",
      "The project will channel water from the country's longest river, the Yangtze, to three rivers in the north, the Yellow, Huai and Hai, whose basins are running dry.\n",
      "\n",
      "이 야심찬 남북 수로 변경 사업은 더 습하고 홍수가 잦은 남부의 강 유역을 매우 건조한 북부 지방과 연결시키는 세 개의 수로를 따라 물을 이동시키게 될 것이다.\n",
      "The ambitious South-to-North Water Transfer Project will move water along three channels linking the wetter flood-prone southern basin to parched northern climes.\n",
      "\n",
      "최근의 사업 중에는 3개의 협곡 수력 발전 댐, 4,000킬로미터의 천연 가스 파이프라인 계획, 그리고 큉하이에서 티벳까지의 세계 최고도의 철로 건설이 포함되어 있다.\n",
      "This is just one of the latest of China's massive public works projects, recent ones including the Three Gorges hydroelectric dam, a proposed 4,000-kilometer natural-gas pipeline and the world's highest railroad from Qinghai to Tibet.\n",
      "\n",
      "수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      "Authorities from the Water Resources Ministry plan to begin construction next year on the controversial and hugely expensive project.\n",
      "\n",
      "장쟈오 수자원부 차관은 사업 승인을 받기 위해 금년말까지 이 사업 계획이 의회에 제출될 것이라고 말했다.\n",
      "Vice Minister of the Water Resources ministry Zhang Jiyao said plans for the project would be submitted for approval to the cabinet by the end of the year.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 한국어와 영어 텍스트 분리하기\n",
    "kor_text = []\n",
    "eng_text = []\n",
    "\n",
    "for sentence in cleaned_corpus:\n",
    "    kor_text.append(sentence[0])\n",
    "    eng_text.append(sentence[1])\n",
    "    \n",
    "\n",
    "print(\"한국어 텍스트 길이:\", len(kor_text))\n",
    "print(\"영어 텍스트 길이:\", len(eng_text))    \n",
    "print()\n",
    "    \n",
    "# 분리된 내용 확인하기\n",
    "for k, e in zip(kor_text[55:61], eng_text[55:61]):\n",
    "    print(k)\n",
    "    print(e)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61660d6d",
   "metadata": {},
   "source": [
    "## 데이터 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "554bd901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence, language='en', s_token=False, e_token=False):\n",
    "    # 앞뒤 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    # 영어의 경우 소문자로 변환\n",
    "    if language == 'en':\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "    # 구두점 주위에 공백 추가\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    \n",
    "    # 연속된 공백을 하나의 공백으로 대체\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    # 언어에 따라 적절한 문자만 유지\n",
    "    if language == 'en':\n",
    "        # 영문자, 숫자, 특정 특수문자만 유지\n",
    "        sentence = re.sub(r\"[^a-zA-Z0-9?.!,]+\", \" \", sentence)\n",
    "    elif language == 'ko':\n",
    "        # 한글, 숫자, 특정 특수문자만 유지\n",
    "        sentence = re.sub(r\"[^가-힣0-9?.!,]+\", \" \", sentence)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language. Use 'en' for English or 'ko' for Korean.\")\n",
    "\n",
    "    # 앞뒤 공백 다시 제거\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    # 시작 토큰 추가 (옵션)\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    # 종료 토큰 추가 (옵션)\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f3ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4e6d514",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 , 960만갤론 이상의 중유를 운반하던 손상된 유조선이 스페인 북서부 연안 130마일 해역에서 침몰하여 , 스페인 당국이 어업이 주요 산업인 이 지역에서 해안선 보호를 위해 황급히 서두르고 있다 .\n",
      "<start> carrying more than 19 . 6 million gallons of heavy fuel oil sank 130 miles off spain s northwest coast , leaving spanish authorities scrambling to protect the coastline in the region where fishing is the primary industry . <end>\n",
      "\n",
      "싣고 있는 기름이 모두 유출된다면 , 알라스카 프린스 윌리암 사운드에서 1989년에 발생했던 엑손 발데즈 기름 유출 사건의 규모보다 두 배 이상이 되는 사상 최대 규모의 기름 유출 사건 중의 하나가 될 것이라고 세계 야생 생물 기금이 경고하고 있다 .\n",
      "<start> the world wildlife fund warns that if all of the oil leaked , it would be one of the largest oil spills ever more than twice the size of the 1989 exxon valdez spill in alaska s prince william sound . <end>\n",
      "\n",
      "대도시를 완전히 파괴할 만큼 커다란 우주의 암석 덩어리들이 지구에 충돌하는 것은 이전에 생각했던 것보다 빈도가 훨씬 적은 , 대략 1천년에 한 번 정도이다 .\n",
      "<start> space boulders big enough to wipe out a major city slam into the earth about once every 1 , 000 years , much less frequently than previously thought . <end>\n",
      "\n",
      "핵폭발의 근거지를 찾아내는 국방성의 인공위성으로부터 얻은 수년간의 자료에 대한 전례 없는 조사에서 이러한 수정된 계산 결과가 나오게 되었다 .\n",
      "<start> the revised calculations come from an unprecedented examination of years of data that scour the ground for nuclear explosions . <end>\n",
      "\n",
      "최인원은 배드민턴 게임을 완벽하게 하고 싶어한다 .\n",
      "<start> choi in won wants to perfect his badminton game . <end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enc_corpus = []\n",
    "dec_corpus = []\n",
    "\n",
    "num_examples = 30000\n",
    "\n",
    "# 한국어 텍스트 전처리\n",
    "for text in kor_text[:num_examples]:\n",
    "    enc_corpus.append(preprocess_sentence(text, language='ko'))\n",
    "\n",
    "# 영어 텍스트 전처리\n",
    "for text in eng_text[:num_examples]:\n",
    "    dec_corpus.append(preprocess_sentence(text, language='en', s_token=True, e_token=True))    \n",
    "    \n",
    "\n",
    "# 전처리된 내용 확인하기\n",
    "for enc, dec in zip(enc_corpus[100:105], dec_corpus[100:105]):\n",
    "    print(enc)\n",
    "    print(dec)\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ec932",
   "metadata": {},
   "source": [
    "## 데이터 전처리: 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95182783",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 20000\n",
    "oov_token = '<unk>'\n",
    "maxlen=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "064020ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', num_words=num_words, oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=maxlen)\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef553741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "def kor_tokenize(corpus):\n",
    "    mecab = Mecab()\n",
    "    tokenized_corpus = [\" \".join(mecab.morphs(sentence)) for sentence in corpus]\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', num_words=num_words, oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=maxlen)\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63433582",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_tensors, enc_tokenizer = eng_tokenize(enc_corpus)\n",
    "dec_tensors, dec_tokenizer = kor_tokenize(dec_corpus)\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(enc_tensors, dec_tensors, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e663a3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 50)\n",
      "(24000, 50)\n",
      "[[    1     1  5257  1194  3136   101 12424     1     1  2017     1   776\n",
      "      1 17681     1    60     2     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [ 5352  7373  5353     9   141    33   654     1     7     2     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [   27   850     1     1    26 12908     3  9244  5214   116  1000     1\n",
      "   9135     1  3403     1     1   343  6691     2     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [  266 11728  4117 14243   495    37  9880     3  5401    37     1   919\n",
      "   3193  8468  3383     1   366     2     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [ 1098  1950  2093  2443   371  1950   375  9633  1923  1378 10548    20\n",
      "    264  1950    23   595 13691   628   375     1     8     2     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]]\n",
      "\n",
      "[[    5   204  1124   782     4   723     4  3529 19276 19277 19278 19279\n",
      "  19280 19281   674 19282 19283   915 14565 14565   223     8  1229  5861\n",
      "     17    43    89    74   250    46   341     6     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [    5   118     9  4685     9   330   374   257    43     9 10295  3010\n",
      "   9084  3414  1047    10   201 10295     3  3010  3414     4     6     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [    5   143    77  9084  3530     3    32    20    60   894    70   438\n",
      "      9  6293     4     6     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [    5  2182 19284  6294     7   475     2 19285   895    21    74   491\n",
      "    246 10296     3  1980     3  2567   163     2   312     4     6     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [    5   866    97    27  2455     9  5207     8  8135  7371    16  1398\n",
      "     10  1683   856    11    31  1379    14     9  3122  4102     8   296\n",
      "    867   591    52   387     8     2    85    12  5208   776    17  3531\n",
      "      7     9  1487   229     4     6     0     0     0     0     0     0\n",
      "      0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(enc_train.shape)\n",
    "print(dec_train.shape)\n",
    "print(enc_train[:5])\n",
    "print()\n",
    "print(dec_tensors[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf08ca",
   "metadata": {},
   "source": [
    "## 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f6c1d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝~\n"
     ]
    }
   ],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f75c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b04babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd561557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (16, 30, 512)\n",
      "Decoder Output: (16, 31433)\n",
      "Decoder Hidden State: (16, 512)\n",
      "Attention: (16, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행하세요.\n",
    "\n",
    "BATCH_SIZE     = 16\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 512  \n",
    "embedding_dim = 256\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338ae0d8",
   "metadata": {},
   "source": [
    "## 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3db0e7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝~\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5a21578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝~\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f187859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm    # tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67170749",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 1500/1500 [05:34<00:00,  4.49it/s, Loss 3.8617]\n",
      "Epoch  2: 100%|██████████| 1500/1500 [04:37<00:00,  5.40it/s, Loss 3.8265]\n",
      "Epoch  3: 100%|██████████| 1500/1500 [04:37<00:00,  5.40it/s, Loss 3.8272]\n",
      "Epoch  4: 100%|██████████| 1500/1500 [04:37<00:00,  5.40it/s, Loss 3.8266]\n",
      "Epoch  5: 100%|██████████| 1500/1500 [04:37<00:00,  5.40it/s, Loss 3.8264]\n",
      "Epoch  6: 100%|██████████| 1500/1500 [04:37<00:00,  5.40it/s, Loss 3.8259]\n",
      "Epoch  7: 100%|██████████| 1500/1500 [04:37<00:00,  5.40it/s, Loss 3.8258]\n",
      "Epoch  8: 100%|██████████| 1500/1500 [04:38<00:00,  5.39it/s, Loss 3.8106]\n",
      "Epoch  9: 100%|██████████| 1500/1500 [04:37<00:00,  5.41it/s, Loss 3.7893]\n",
      "Epoch 10: 100%|██████████| 1500/1500 [04:37<00:00,  5.41it/s, Loss 3.7882]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)    # tqdm\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))    # tqdm\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))    # tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e05c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define eval_step\n",
    "\n",
    "@tf.function\n",
    "def eval_step(src, tgt, encoder, decoder, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    enc_out = encoder(src)\n",
    "\n",
    "    h_dec = enc_out[:, -1]\n",
    "    \n",
    "    dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "    for t in range(1, tgt.shape[1]):\n",
    "        pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "        loss += loss_function(tgt[:, t], pred)\n",
    "        dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "    \n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "# Training Process\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_val.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (test_batch, idx) in enumerate(t):\n",
    "        test_batch_loss = eval_step(enc_val[idx:idx+BATCH_SIZE],\n",
    "                                    dec_val[idx:idx+BATCH_SIZE],\n",
    "                                    encoder,\n",
    "                                    decoder,\n",
    "                                    dec_tokenizer)\n",
    "    \n",
    "        test_loss += test_batch_loss\n",
    "\n",
    "        t.set_description_str('Test Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Test Loss %.4f' % (test_loss.numpy() / (test_batch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f36741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41ed5fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: . .\n",
      "Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_139/2860932276.py:45: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "/tmp/ipykernel_139/2860932276.py:46: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 573,
       "width": 66
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(\"안녕하세요. 만나서 반갑습니다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3fadd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: .\n",
      "Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_139/2860932276.py:45: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "/tmp/ipykernel_139/2860932276.py:46: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 573,
       "width": 55
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(\"이 야심찬 남북 수로 변경 사업은 더 습하고 홍수가 잦은 남부의 강 유역을 매우 건조한 북부 지방과 연결시키는 세 개의 수로를 따라 물을 이동시키게 될 것이다.\", encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
